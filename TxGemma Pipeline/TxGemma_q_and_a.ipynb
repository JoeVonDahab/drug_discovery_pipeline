{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48a30aa",
      "metadata": {
        "id": "b48a30aa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.version.cuda)       # should report “12.8” (you might have to downgrade your pip install torch version from requirements.txt depending on what gpu you have)\n",
        "print(torch.cuda.is_available())  # should be True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "PMmpJHpUClpZ"
      },
      "id": "PMmpJHpUClpZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "89eo-8TdCxNb"
      },
      "id": "89eo-8TdCxNb"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet accelerate bitsandbytes huggingface_hub transformers"
      ],
      "metadata": {
        "id": "7nSJ2DusCnVm"
      },
      "id": "7nSJ2DusCnVm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the prediction and chat model from Hugging Face Hub"
      ],
      "metadata": {
        "id": "Ui_QEg7JC34J"
      },
      "id": "Ui_QEg7JC34J"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "PREDICT_VARIANT = \"2b-predict\"  # @param [\"2b-predict\", \"9b-predict\", \"27b-predict\"]\n",
        "CHAT_VARIANT = \"9b-chat\" # @param [\"9b-chat\", \"27b-chat\"]\n",
        "USE_CHAT = True # @param {type: \"boolean\"}\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "predict_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{PREDICT_VARIANT}\")\n",
        "predict_model = AutoModelForCausalLM.from_pretrained(\n",
        "    f\"google/txgemma-{PREDICT_VARIANT}\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "\n",
        "if USE_CHAT:\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{CHAT_VARIANT}\")\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        f\"google/txgemma-{CHAT_VARIANT}\",\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "    )"
      ],
      "metadata": {
        "id": "lwHPBz8CCy_w"
      },
      "id": "lwHPBz8CCy_w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looking at TxGemma's prompt format"
      ],
      "metadata": {
        "id": "VX7I_w8OmDK6"
      },
      "id": "VX7I_w8OmDK6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "TxGemma requires a specific input format since it was fine-tuned against TDC, so these lines are just used to visualize what the prompt structure is."
      ],
      "metadata": {
        "id": "LT7IcI4zmJ25"
      },
      "id": "LT7IcI4zmJ25"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "tdc_prompts_filepath = hf_hub_download(\n",
        "    repo_id=\"google/txgemma-2b-predict\",\n",
        "    filename=\"tdc_prompts.json\",\n",
        ")\n",
        "\n",
        "with open(tdc_prompts_filepath, \"r\") as f:\n",
        "    tdc_prompts_json = json.load(f)"
      ],
      "metadata": {
        "id": "v6iXPtqzDeXr"
      },
      "id": "v6iXPtqzDeXr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tdc_prompts_json[task_name]"
      ],
      "metadata": {
        "id": "n9NlZhHQEBNk"
      },
      "id": "n9NlZhHQEBNk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the prediction and chat model"
      ],
      "metadata": {
        "id": "DOGg160cDPAV"
      },
      "id": "DOGg160cDPAV"
    },
    {
      "cell_type": "code",
      "source": [
        "## Example SMILE taken from Miko's compiled dataset for testing purposes\n",
        "SMILES = \"C1CCN(C1)C2=CC=CC=C2NC3=NS(=O)(=O)C4=CC=CC=C43\"\n",
        "\n",
        "HIF_PROMPT = f\"\"\"\n",
        "Instructions: Answer the following question about ligand–protein binding.\n",
        "\n",
        "Context: Hypoxia‐inducible factor 2α (HIF-2α) is a transcription factor whose activity depends on specific ligand binding.\n",
        "Key properties influencing binding include measured or predicted binding affinity, lipophilicity (pLogP), and chemical\n",
        "similarity to known HIF-2α binders.\n",
        "\n",
        "Question: Given a ligand’s SMILES string below, predict whether it\n",
        "  (A) does NOT bind HIF-2α\n",
        "  (B) does bind HIF-2α\n",
        "\n",
        "Ligand SMILES: {SMILES}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Fine tuning TxGemma is required\n",
        "def txgemma_predict(prompt):\n",
        "    input_ids = predict_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = predict_model.generate(**input_ids, max_new_tokens=8)\n",
        "    return predict_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "#\n",
        "def txgemma_chat(prompt):\n",
        "    input_ids = chat_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = chat_model.generate(**input_ids, max_new_tokens=32)\n",
        "    return chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Prediction model response: {txgemma_predict(HIF_PROMPT)}\")\n",
        "print(\"=================================================================\")\n",
        "if USE_CHAT: print(f\"Chat model response: {txgemma_chat(HIF_PROMPT)}\")"
      ],
      "metadata": {
        "id": "Awy1IYpJDGtg"
      },
      "id": "Awy1IYpJDGtg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0UDMuXXUDg67"
      },
      "id": "0UDMuXXUDg67",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cdd223",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}