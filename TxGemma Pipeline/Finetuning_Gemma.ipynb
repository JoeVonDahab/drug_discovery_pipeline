{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc23d95",
      "metadata": {
        "id": "0fc23d95"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce7c2b59",
      "metadata": {
        "id": "ce7c2b59"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet bitsandbytes datasets peft transformers trl rdkit"
      ],
      "metadata": {
        "id": "8Xi3v5kRDydu"
      },
      "id": "8Xi3v5kRDydu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "537afddc",
      "metadata": {
        "id": "537afddc"
      },
      "source": [
        "# Load model from HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970a87b2",
      "metadata": {
        "id": "970a87b2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model = \"google/txgemma-\"\n",
        "CHAT_VARIANT = \"9b-chat\" # @param [\"9b-chat\", \"27b-chat\"]\n",
        "\n",
        "model_id = base_model + CHAT_VARIANT\n",
        "\n",
        "# Use 4-bit quantization to reduce memory usage\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map={\"\":0},\n",
        "    torch_dtype=\"auto\",\n",
        "    attn_implementation=\"eager\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b127cb8e",
      "metadata": {
        "id": "b127cb8e"
      },
      "source": [
        "# Load Dataset and Clean It"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Known Binders (taken from BindingDB curated by UCSD)"
      ],
      "metadata": {
        "id": "Ei-xCvGnv8ZG"
      },
      "id": "Ei-xCvGnv8ZG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c25db361",
      "metadata": {
        "id": "c25db361"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Known_HIF_Binders.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = df[[\"Ligand SMILES\", \"IC50 (nM)\"]].dropna()\n",
        "\n",
        "# remove rows that contain '<' or '>'\n",
        "has_censor = clean_df[\"IC50 (nM)\"] \\\n",
        "    .astype(str) \\\n",
        "    .str.contains(r\"[<>]\")\n",
        "\n",
        "# count how many rows will be dropped\n",
        "dropped_count = has_censor.sum()\n",
        "print(f\"Dropping {dropped_count} rows with '<' or '>' in IC50\")\n",
        "\n",
        "# keep only the rows *without* '<' or '>'\n",
        "clean_df = clean_df.loc[~has_censor].reset_index(drop=True)\n",
        "clean_df"
      ],
      "metadata": {
        "id": "-sHka3yIJ-bf"
      },
      "id": "-sHka3yIJ-bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Crippen, Lipinski\n",
        "\n",
        "# --- 2) Clean & standardize IC50, compute pIC50 ---\n",
        "def parse_ic50_to_pic50(ic50_str):\n",
        "    \"\"\"Convert a string like '<5' or '200' (in nM) to pIC50.\"\"\"\n",
        "    # strip any whitespace\n",
        "    s = str(ic50_str).strip()\n",
        "    try:\n",
        "        nm = float(s)\n",
        "    except ValueError:\n",
        "        return np.nan  # unparseable\n",
        "\n",
        "    # convert nM â†’ M\n",
        "    m = nm * 1e-9\n",
        "    # pIC50\n",
        "    pic50 = -np.log10(m)\n",
        "    return pic50\n",
        "\n",
        "clean_df[\"pIC50\"] = clean_df[\"IC50 (nM)\"].apply(parse_ic50_to_pic50)\n",
        "\n",
        "# --- 3) Bin into activity classes ---\n",
        "# strong binder if pIC50 â‰¥ 7 (IC50 â‰¤ 100 nM), else weak/non-binder\n",
        "threshold = 7.0\n",
        "clean_df[\"activity_class\"] = np.where(clean_df[\"pIC50\"] >= threshold, \"strong\", \"weak\")\n",
        "\n",
        "# --- 4) Compute 2D descriptors via RDKit ---\n",
        "def compute_descriptors(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return {\n",
        "            \"MolWt\": np.nan,\n",
        "            \"TPSA\": np.nan,\n",
        "            \"HBD\": np.nan,\n",
        "            \"HBA\": np.nan,\n",
        "            \"RotBonds\": np.nan,\n",
        "            \"LogP\": np.nan,\n",
        "        }\n",
        "    return {\n",
        "        \"MolWt\": Descriptors.MolWt(mol),\n",
        "        \"TPSA\": Descriptors.TPSA(mol),\n",
        "        \"HBD\": Lipinski.NumHDonors(mol),\n",
        "        \"HBA\": Lipinski.NumHAcceptors(mol),\n",
        "        \"RotBonds\": Descriptors.NumRotatableBonds(mol),\n",
        "        \"LogP\": Crippen.MolLogP(mol),\n",
        "    }\n",
        "\n",
        "# apply and expand into separate columns\n",
        "desc_df = clean_df[\"Ligand SMILES\"].apply(compute_descriptors).apply(pd.Series)\n",
        "clean_df = pd.concat([clean_df, desc_df], axis=1)\n",
        "clean_df[\"is_known_binder\"] = True\n",
        "\n",
        "# --- 5) View the table ---\n",
        "print(clean_df.head())"
      ],
      "metadata": {
        "id": "hA2w5RjnXeMZ"
      },
      "id": "hA2w5RjnXeMZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duds (Taken from BindingDB on Compounds that Bind to ESR1)"
      ],
      "metadata": {
        "id": "3xoQB-q5v-zM"
      },
      "id": "3xoQB-q5v-zM"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"ESR1_Binders.csv\")"
      ],
      "metadata": {
        "id": "bG3gxKzTwqSc"
      },
      "id": "bG3gxKzTwqSc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esr1_df = df[[\"Ligand SMILES\", \"IC50 (nM)\"]].dropna()\n",
        "\n",
        "# remove rows that contain '<' or '>'\n",
        "has_censor = esr1_df[\"IC50 (nM)\"] \\\n",
        "    .astype(str) \\\n",
        "    .str.contains(r\"[<>]\")\n",
        "\n",
        "# count how many rows will be dropped\n",
        "dropped_count = has_censor.sum()\n",
        "print(f\"Dropping {dropped_count} rows with '<' or '>' in IC50\")\n",
        "\n",
        "# keep only the rows *without* '<' or '>'\n",
        "esr1_df = esr1_df.loc[~has_censor].reset_index(drop=True)\n",
        "esr1_df.reset_index(drop=True, inplace=True)\n",
        "esr1_df"
      ],
      "metadata": {
        "id": "ZqUT4mArEh6h"
      },
      "id": "ZqUT4mArEh6h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esr1_df[\"pIC50\"] = esr1_df[\"IC50 (nM)\"].apply(parse_ic50_to_pic50)\n",
        "\n",
        "esr1_df[\"activity_class\"] = np.where(esr1_df[\"pIC50\"] >= threshold, \"strong\", \"weak\")\n",
        "\n",
        "\n",
        "# apply and expand into separate columns\n",
        "temp_df = esr1_df[\"Ligand SMILES\"].apply(compute_descriptors).apply(pd.Series)\n",
        "esr1_df = pd.concat([esr1_df, temp_df], axis=1)\n",
        "esr1_df[\"is_known_binder\"] = False\n",
        "\n",
        "\n",
        "print(esr1_df.head())"
      ],
      "metadata": {
        "id": "8I30Ct05Emxc"
      },
      "id": "8I30Ct05Emxc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining the known and unknown binders"
      ],
      "metadata": {
        "id": "MB3Pg8QxHYbo"
      },
      "id": "MB3Pg8QxHYbo"
    },
    {
      "cell_type": "code",
      "source": [
        "all_binders = pd.concat([clean_df, esr1_df], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "f_mGLgAZFmKI"
      },
      "id": "f_mGLgAZFmKI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perm = np.random.permutation(len(all_binders))\n",
        "all_binders = all_binders.iloc[perm].reset_index(drop=True)\n",
        "\n",
        "all_binders.to_csv(\"all_binders.csv\", index=False)"
      ],
      "metadata": {
        "id": "ByHYaN1SIp3n"
      },
      "id": "ByHYaN1SIp3n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning the model (finally ðŸ˜±)"
      ],
      "metadata": {
        "id": "p21qqvZSI-ol"
      },
      "id": "p21qqvZSI-ol"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sL3vXhbTI2Xe"
      },
      "id": "sL3vXhbTI2Xe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}