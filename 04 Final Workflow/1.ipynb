{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a377b2",
   "metadata": {},
   "source": [
    "input: diffdock_before.txt, receptor_clean.pdb\n",
    "\n",
    "output: Lipinski_before.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert smiles string to sdf\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"sdf_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read SMILES from file\n",
    "with open(\"diffdock_before.txt\", \"r\") as file:\n",
    "    smiles_list = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Generate SDF files\n",
    "for idx, smiles in enumerate(smiles_list):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"Skipping invalid SMILES at line {idx + 1}: {smiles}\")\n",
    "        continue\n",
    "    mol = Chem.AddHs(mol)\n",
    "    if AllChem.EmbedMolecule(mol, AllChem.ETKDG()) != 0:\n",
    "        print(f\"Embedding failed for SMILES at line {idx + 1}: {smiles}\")\n",
    "        continue\n",
    "    AllChem.UFFOptimizeMolecule(mol)\n",
    "    \n",
    "    sdf_path = os.path.join(output_dir, f\"ligand_{idx}.sdf\")\n",
    "    writer = Chem.SDWriter(sdf_path)\n",
    "    writer.write(mol)\n",
    "    writer.close()\n",
    "\n",
    "print(f\"Finished writing {len(smiles_list)} SDF files to '{output_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4acb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run diffdock\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- CONFIG ----\n",
    "input_dir = \"sdf_output\"\n",
    "output_dir = \"results_output\"\n",
    "receptor_path = \"receptor_clean.pdb\"\n",
    "\n",
    "url = \"https://health.api.nvidia.com/v1/biology/mit/diffdock\"\n",
    "header_auth = \"Bearer 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\"\n",
    "\n",
    "# ---- ASSET UPLOAD FUNCTION ----\n",
    "def _upload_asset(input_data):\n",
    "    assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\"\n",
    "    headers = {\n",
    "        \"Authorization\": header_auth,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"accept\": \"application/json\",\n",
    "    }\n",
    "    s3_headers = {\n",
    "        \"x-amz-meta-nvcf-asset-description\": \"diffdock-file\",\n",
    "        \"content-type\": \"text/plain\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"contentType\": \"text/plain\",\n",
    "        \"description\": \"diffdock-file\"\n",
    "    }\n",
    "\n",
    "    for attempt in range(5):  # retry up to 5 times\n",
    "        try:\n",
    "            response = requests.post(assets_url, headers=headers, json=payload, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            asset_url = response.json()[\"uploadUrl\"]\n",
    "            asset_id = response.json()[\"assetId\"]\n",
    "\n",
    "            response = requests.put(asset_url, data=input_data, headers=s3_headers, timeout=300)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            return asset_id\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                wait = 2 ** attempt + random.uniform(0, 1)\n",
    "                print(f\"[WARN] Rate limited. Retrying after {wait:.2f}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise\n",
    "    raise RuntimeError(\"Failed to upload asset after multiple attempts\")\n",
    "\n",
    "# ---- UPLOAD PROTEIN ONCE ----\n",
    "with open(receptor_path, \"rb\") as f:\n",
    "    protein_id = _upload_asset(f.read())\n",
    "print(f\"Protein uploaded: {protein_id}\")\n",
    "\n",
    "# ---- PROCESS ONE LIGAND ----\n",
    "def process_ligand(idx, sdf_file):\n",
    "    try:\n",
    "        ligand_path = os.path.join(input_dir, sdf_file)\n",
    "        out_folder = os.path.join(output_dir, f\"ligand_{idx}\")\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "        with open(ligand_path, \"rb\") as f:\n",
    "            ligand_id = _upload_asset(f.read())\n",
    "\n",
    "        print(f\"Ligand {sdf_file} uploaded: {ligand_id}\")\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"NVCF-INPUT-ASSET-REFERENCES\": f\"{protein_id},{ligand_id}\",\n",
    "            \"Authorization\": header_auth\n",
    "        }\n",
    "\n",
    "        payload = {\n",
    "            \"ligand\": ligand_id,\n",
    "            \"ligand_file_type\": \"sdf\",\n",
    "            \"protein\": protein_id,\n",
    "            \"num_poses\": 20,\n",
    "            \"time_divisions\": 20,\n",
    "            \"steps\": 18,\n",
    "            \"save_trajectory\": True,\n",
    "            \"is_staged\": True\n",
    "        }\n",
    "\n",
    "        for attempt in range(5):  # Retry logic for rate-limited inference\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            if response.status_code != 429:\n",
    "                break\n",
    "            wait = 2 ** attempt + random.uniform(0, 1)\n",
    "            print(f\"[WARN] Inference rate-limited. Retrying after {wait:.2f}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        with open(os.path.join(out_folder, \"response_status.txt\"), \"w\") as f:\n",
    "            f.write(str(response))\n",
    "\n",
    "        with open(os.path.join(out_folder, \"request_url.txt\"), \"w\") as f:\n",
    "            f.write(url)\n",
    "\n",
    "        with open(os.path.join(out_folder, \"response_text.txt\"), \"w\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "        print(f\"Completed ligand_{idx}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ligand {idx} failed: {e}\")\n",
    "\n",
    "# ---- MULTITHREADING EXECUTION ----\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "sdf_files = [f for f in os.listdir(input_dir) if f.endswith(\".sdf\")]\n",
    "\n",
    "# Reduce concurrency to avoid 429 errors\n",
    "max_workers = min(3, len(sdf_files))  # Try 2â€“3 threads instead of 8\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_ligand, idx, sdf_file) for idx, sdf_file in enumerate(sdf_files)]\n",
    "    for future in as_completed(futures):\n",
    "        pass  # Ensures we wait for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f8a46",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\zhaol\\\\Downloads\\\\drug_discovery_pipeline\\\\Docking Pipeline\\\\results_output\\\\top_100_ligand_confidences.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Save ligand numbers and confidence scores\u001b[39;00m\n\u001b[0;32m     48\u001b[0m conf_out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_100_ligand_confidences.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconf_out_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[0;32m     50\u001b[0m     out\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLigand Number\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mConfidence Score\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m num, score \u001b[38;5;129;01min\u001b[39;00m top_100_ligands:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\zhaol\\\\Downloads\\\\drug_discovery_pipeline\\\\Docking Pipeline\\\\results_output\\\\top_100_ligand_confidences.txt'"
     ]
    }
   ],
   "source": [
    "#reformatting the output. select the top 100 based on confidence score. extract their smiles string.\n",
    "import json\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "base_path = r\"results_output\"\n",
    "ligand_confidences = []\n",
    "\n",
    "for i in range(40000):\n",
    "    ligand_folder = os.path.join(base_path, f\"ligand_{i}\")\n",
    "    input_file = os.path.join(ligand_folder, \"response_text.txt\")\n",
    "    output_folder = os.path.join(ligand_folder, \"diffdock_actual_outcome\")\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Write PDB and SDF\n",
    "    for j, pose in enumerate(data.get(\"trajectory\", []), start=1):\n",
    "        with open(os.path.join(output_folder, f\"pose_{j}.pdb\"), \"w\") as pdb_file:\n",
    "            pdb_file.write(pose)\n",
    "\n",
    "    for j, sdf in enumerate(data.get(\"ligand_positions\", []), start=1):\n",
    "        with open(os.path.join(output_folder, f\"ligand_pose_{j}.sdf\"), \"w\") as sdf_file:\n",
    "            sdf_file.write(sdf)\n",
    "\n",
    "    # Write confidence scores\n",
    "    confidences = data.get(\"position_confidence\", [])\n",
    "    with open(os.path.join(output_folder, \"pose_confidences.txt\"), \"w\") as out_file:\n",
    "        out_file.write(\"Rank \\t Pose Confidence\\n\\n\")\n",
    "        for j, conf in enumerate(confidences, start=1):\n",
    "            out_file.write(f\"{j} \\t {conf}\\n\")\n",
    "\n",
    "    valid_confidences = [c for c in confidences if c is not None]\n",
    "    if valid_confidences:\n",
    "        highest = max(valid_confidences)\n",
    "        ligand_confidences.append((i, highest))\n",
    "\n",
    "# Sort and select top 100 ligands\n",
    "ligand_confidences.sort(key=lambda x: x[1], reverse=True)\n",
    "top_100_ligands = ligand_confidences[:100]\n",
    "\n",
    "# Save ligand numbers and confidence scores\n",
    "conf_out_path = os.path.join(base_path, \"top_100_ligand_confidences.txt\")\n",
    "with open(conf_out_path, \"w\") as out:\n",
    "    out.write(\"Ligand Number\\tConfidence Score\\n\")\n",
    "    for num, score in top_100_ligands:\n",
    "        out.write(f\"{num}\\t{score:.4f}\\n\")\n",
    "\n",
    "# Convert PDB to SMILES and save\n",
    "smiles_out_path = os.path.join(base_path, \"Lipinski_before.txt\")\n",
    "with open(smiles_out_path, \"w\") as out:\n",
    "    out.write(\"Ligand Number\\tSMILES\\n\")\n",
    "    for num, _ in top_100_ligands:\n",
    "        pdb_path = os.path.join(base_path, f\"ligand_{num}\", \"diffdock_actual_outcome\", \"ligand_pose_1.pdb\")\n",
    "        if os.path.exists(pdb_path):\n",
    "            mol = Chem.MolFromPDBFile(pdb_path, removeHs=False)\n",
    "            if mol is not None:\n",
    "                try:\n",
    "                    AllChem.Compute2DCoords(mol)\n",
    "                    smiles = Chem.MolToSmiles(mol)\n",
    "                    out.write(f\"{num}\\t{smiles}\\n\")\n",
    "                except:\n",
    "                    out.write(f\"{num}\\tERROR: Failed to generate SMILES\\n\")\n",
    "            else:\n",
    "                out.write(f\"{num}\\tERROR: Invalid molecule\\n\")\n",
    "        else:\n",
    "            out.write(f\"{num}\\tERROR: PDB file missing\\n\")\n",
    "\n",
    "print(\"Output written to:\")\n",
    "print(conf_out_path)\n",
    "print(smiles_out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c56ff",
   "metadata": {},
   "source": [
    "#\n",
    "input: Lipinski_before.txt\n",
    "output:Lipinski_after.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676100a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 40924 molecules that passed Lipinskiâ€™s rule to `Lipinski_after.txt`\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "def lipinski_violations(mol):\n",
    "    \"\"\"Return the count of Lipinski rule violations.\"\"\"\n",
    "    mw   = Descriptors.MolWt(mol)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    hbd  = Descriptors.NumHDonors(mol)\n",
    "    hba  = Descriptors.NumHAcceptors(mol)\n",
    "\n",
    "    violations = 0\n",
    "    if mw   >= 500: violations += 1\n",
    "    if logp >= 5:   violations += 1\n",
    "    if hbd  >= 5:   violations += 1\n",
    "    if hba  >= 10:  violations += 1\n",
    "\n",
    "    return violations\n",
    "\n",
    "def filter_lipinski(input_path='Lipinski_before.txt', output_path='Lipinski_after.txt'):\n",
    "    passed = []\n",
    "    with open(input_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            smi = line.strip()\n",
    "            if not smi:\n",
    "                continue\n",
    "\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                continue\n",
    "\n",
    "            if lipinski_violations(mol) <= 1:\n",
    "                passed.append(smi)\n",
    "\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        for smi in passed:\n",
    "            outfile.write(smi + '\\n')\n",
    "\n",
    "    print(f\"Saved {len(passed)} molecules that passed Lipinskiâ€™s rule to `{output_path}`\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filter_lipinski()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b26fb6",
   "metadata": {},
   "source": [
    "check binding pocket via autodock"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
