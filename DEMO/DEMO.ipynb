{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bed6f44",
   "metadata": {},
   "source": [
    "optional: expand current library using Molmim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7effc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69836a21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "input: library.txt\n",
    "output: diffdock_before.txt\n",
    "\n",
    "Need: API key from Molmim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968575b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report written to library_2.txt\n",
      "0 invalid SMILES removed from library.txt\n"
     ]
    }
   ],
   "source": [
    "#verify the smiles in library.txt\n",
    "import sys\n",
    "from rdkit import Chem\n",
    "\n",
    "def is_valid_smiles(smiles: str) -> bool:\n",
    "    return Chem.MolFromSmiles(smiles) is not None\n",
    "\n",
    "def main(input_path: str, report_path: str = None):\n",
    "    out = open(report_path, 'w') if report_path else sys.stdout\n",
    "    valid_smiles = []\n",
    "    valid_count = 0\n",
    "    invalid_count = 0\n",
    "\n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for lineno, line in enumerate(lines, 1):\n",
    "        original_line = line.strip()\n",
    "        if not original_line or original_line.startswith('#'):\n",
    "            continue\n",
    "        smiles = original_line.split()[0]  # Assume first token is SMILES\n",
    "        valid = is_valid_smiles(smiles)\n",
    "        status = \"VALID\" if valid else \"INVALID\"\n",
    "        out.write(f\"{lineno:4d}  {smiles:20s}  {status:7s}\\n\")\n",
    "        \n",
    "        if valid:\n",
    "            valid_smiles.append(original_line)\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            invalid_count += 1\n",
    "\n",
    "    # Overwrite input file with valid SMILES only\n",
    "    with open(input_path, 'w') as f:\n",
    "        for line in valid_smiles:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    out.write(f\"\\nSummary:\\n\")\n",
    "    out.write(f\"Valid SMILES: {valid_count}\\n\")\n",
    "    out.write(f\"Invalid SMILES: {invalid_count}\\n\")\n",
    "    out.write(f\"Total SMILES: {valid_count + invalid_count}\\n\")\n",
    "\n",
    "    if report_path:\n",
    "        out.close()\n",
    "        print(f\"Validation report written to {report_path}\")\n",
    "    print(f\"{invalid_count} invalid SMILES removed from {input_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inp = \"library.txt\"\n",
    "    rpt = \"library_2.txt\"\n",
    "    main(inp, rpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bb692b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060 new SMILES strings generated and written to 'diffdock_before.txt'.\n"
     ]
    }
   ],
   "source": [
    "#expand library via Molmim\n",
    "import os\n",
    "import ast\n",
    "import requests\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.QED import qed as rdkit_qed\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "from rdkit.DataStructs import TanimotoSimilarity\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # --- Load API key ---\n",
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"API_KEY\")\n",
    "# if not API_KEY:\n",
    "#     raise ValueError(\"API_KEY is not set in the .env file.\")\n",
    "\n",
    "# --- Set API key directly ---\n",
    "API_KEY = \"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\"\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API_KEY is not set.\")\n",
    "\n",
    "\n",
    "# --- API setup ---\n",
    "invoke_url = \"https://health.api.nvidia.com/v1/biology/nvidia/molmim/generate\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Accept\": \"application/json\"}\n",
    "session = requests.Session()\n",
    "\n",
    "# --- Tanimoto similarity (optional if you want to sort or filter) ---\n",
    "def tanimoto_similarity(smiles1, smiles2):\n",
    "    mol1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol2 = Chem.MolFromSmiles(smiles2)\n",
    "    if not mol1 or not mol2:\n",
    "        return 0.0\n",
    "    fp1 = GetMorganFingerprintAsBitVect(mol1, 2, nBits=2048)\n",
    "    fp2 = GetMorganFingerprintAsBitVect(mol2, 2, nBits=2048)\n",
    "    return TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "# --- Process a single SMILES via the API ---\n",
    "def generate_optimized_smiles(original_smiles):\n",
    "    if not Chem.MolFromSmiles(original_smiles):\n",
    "        print(f\"Invalid SMILES: {original_smiles}\")\n",
    "        return []\n",
    "\n",
    "    generated_set = set()\n",
    "    min_sims = [0.1, 0.4, 0.7]\n",
    "\n",
    "    for min_sim in min_sims:\n",
    "        payload = {\n",
    "            \"smi\": original_smiles,\n",
    "            \"algorithm\": \"CMA-ES\",\n",
    "            \"num_molecules\": 10,\n",
    "            \"property_name\": \"QED\",\n",
    "            \"minimize\": False,\n",
    "            \"min_similarity\": min_sim,\n",
    "            \"particles\": 20,\n",
    "            \"iterations\": 2,\n",
    "            \"scaled_radius\": 1,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = session.post(invoke_url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            molecules = ast.literal_eval(response.json().get('molecules', '[]'))\n",
    "            for mol in molecules:\n",
    "                gen = mol.get('sample')\n",
    "                if gen and Chem.MolFromSmiles(gen):\n",
    "                    canonical = Chem.MolToSmiles(Chem.MolFromSmiles(gen), canonical=True)\n",
    "                    generated_set.add(canonical)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during API request for SMILES '{original_smiles}': {e}\")\n",
    "            continue\n",
    "\n",
    "    return list(generated_set)\n",
    "\n",
    "# --- Main processing block ---\n",
    "def main(input_file=\"library.txt\", output_file=\"diffdock_before.txt\"):\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Input file '{input_file}' not found.\")\n",
    "        return\n",
    "\n",
    "    total_added = 0\n",
    "    all_generated = []\n",
    "\n",
    "    with open(input_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            smiles = line.strip()\n",
    "            if not smiles or smiles.startswith('#'):\n",
    "                continue\n",
    "            new_smiles = generate_optimized_smiles(smiles)\n",
    "            total_added += len(new_smiles)\n",
    "            all_generated.extend(new_smiles)\n",
    "\n",
    "    # Remove duplicates\n",
    "    all_generated = sorted(set(all_generated))\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for smi in all_generated:\n",
    "            outfile.write(f\"{smi}\\n\")\n",
    "\n",
    "    print(f\"{total_added} new SMILES strings generated and written to '{output_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7212fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report written to library_4.txt\n",
      "0 invalid SMILES removed from diffdock_before.txt\n"
     ]
    }
   ],
   "source": [
    "#verify the smiles in library.txt\n",
    "import sys\n",
    "from rdkit import Chem\n",
    "\n",
    "def is_valid_smiles(smiles: str) -> bool:\n",
    "    return Chem.MolFromSmiles(smiles) is not None\n",
    "\n",
    "def main(input_path: str, report_path: str = None):\n",
    "    out = open(report_path, 'w') if report_path else sys.stdout\n",
    "    valid_smiles = []\n",
    "    valid_count = 0\n",
    "    invalid_count = 0\n",
    "\n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for lineno, line in enumerate(lines, 1):\n",
    "        original_line = line.strip()\n",
    "        if not original_line or original_line.startswith('#'):\n",
    "            continue\n",
    "        smiles = original_line.split()[0]  # Assume first token is SMILES\n",
    "        valid = is_valid_smiles(smiles)\n",
    "        status = \"VALID\" if valid else \"INVALID\"\n",
    "        out.write(f\"{lineno:4d}  {smiles:20s}  {status:7s}\\n\")\n",
    "        \n",
    "        if valid:\n",
    "            valid_smiles.append(original_line)\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            invalid_count += 1\n",
    "\n",
    "    # Overwrite input file with valid SMILES only\n",
    "    with open(input_path, 'w') as f:\n",
    "        for line in valid_smiles:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    out.write(f\"\\nSummary:\\n\")\n",
    "    out.write(f\"Valid SMILES: {valid_count}\\n\")\n",
    "    out.write(f\"Invalid SMILES: {invalid_count}\\n\")\n",
    "    out.write(f\"Total SMILES: {valid_count + invalid_count}\\n\")\n",
    "\n",
    "    if report_path:\n",
    "        out.close()\n",
    "        print(f\"Validation report written to {report_path}\")\n",
    "    print(f\"{invalid_count} invalid SMILES removed from {input_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inp = \"diffdock_before.txt\"\n",
    "    rpt = \"library_4.txt\"\n",
    "    main(inp, rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02ef5c",
   "metadata": {},
   "source": [
    "step 1: Docking (DiffDock)\n",
    "\n",
    "input: diffdock_before.txt, receptor_clean.pdb\n",
    "\n",
    "output: Lipinski_before.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acf05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing 36 SDF files to 'sdf_output'\n"
     ]
    }
   ],
   "source": [
    "#convert smiles string to sdf\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"sdf_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read SMILES from file\n",
    "with open(\"diffdock_before.txt\", \"r\") as file:\n",
    "    smiles_list = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Generate SDF files\n",
    "for idx, smiles in enumerate(smiles_list):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"Skipping invalid SMILES at line {idx + 1}: {smiles}\")\n",
    "        continue\n",
    "    mol = Chem.AddHs(mol)\n",
    "    if AllChem.EmbedMolecule(mol, AllChem.ETKDG()) != 0:\n",
    "        print(f\"Embedding failed for SMILES at line {idx + 1}: {smiles}\")\n",
    "        continue\n",
    "    AllChem.UFFOptimizeMolecule(mol)\n",
    "    \n",
    "    sdf_path = os.path.join(output_dir, f\"ligand_{idx}.sdf\")\n",
    "    writer = Chem.SDWriter(sdf_path)\n",
    "    writer.write(mol)\n",
    "    writer.close()\n",
    "\n",
    "print(f\"Finished writing {len(smiles_list)} SDF files to '{output_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413c201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein uploaded: de5d3702-1f31-4ed0-a87b-2d7f3156ec6c\n",
      "Ligand ligand_1.sdf uploaded: 567faa38-e86b-48d8-bd15-180f9cc7d9de\n",
      "Ligand ligand_0.sdf uploaded: 35f0c1e4-8539-4cee-a6ae-58e09806a3da\n",
      "Ligand ligand_10.sdf uploaded: 04e3ce23-5504-481d-a180-126222e377b6\n",
      "Completed ligand_0: 200\n",
      "Completed ligand_2: 200\n",
      "Completed ligand_1: 200\n",
      "Ligand ligand_11.sdf uploaded: ca2b1e33-3bcd-407c-9948-64c89d581eb5\n",
      "Ligand ligand_13.sdf uploaded: 7691c5ae-bd60-4122-b7fa-4df9b3811d5b\n",
      "Ligand ligand_12.sdf uploaded: d0c5bccf-7808-4952-8f5e-d8a245dde667\n",
      "Completed ligand_3: 200\n",
      "Completed ligand_5: 200\n",
      "Completed ligand_4: 200\n",
      "Ligand ligand_14.sdf uploaded: 28f1c181-8340-446b-a1fc-e65bacb6d705\n",
      "Ligand ligand_15.sdf uploaded: a83f8f5c-98d4-4f44-8de3-907a655b884e\n",
      "Ligand ligand_16.sdf uploaded: b09540a1-4e02-4f70-9291-ed14a02ca14b\n",
      "Completed ligand_6: 200\n",
      "Completed ligand_7: 200\n",
      "Completed ligand_8: 200\n",
      "Ligand ligand_17.sdf uploaded: 97f51610-1903-498e-ae60-f9bde52f6f4a\n",
      "Ligand ligand_18.sdf uploaded: 85f251b2-d00d-4808-a38d-0ad93175323c\n",
      "Ligand ligand_19.sdf uploaded: 4f2d8097-74c4-4ea3-9e0e-68502f54b5f8\n",
      "Completed ligand_10: 200\n",
      "Completed ligand_9: 200\n",
      "Completed ligand_11: 200\n",
      "Ligand ligand_2.sdf uploaded: 7fe824f1-c926-40ac-94fd-ef83dc210776\n",
      "Ligand ligand_20.sdf uploaded: bee21139-4cc6-4a19-b508-96c74b3a2610\n",
      "Ligand ligand_21.sdf uploaded: 60e7fd6b-122f-4768-9663-f16229ce72ab\n",
      "Completed ligand_13: 200\n",
      "Completed ligand_12: 200\n",
      "Completed ligand_14: 200\n",
      "Ligand ligand_22.sdf uploaded: 88b9e057-984d-4d2a-839d-f250e11c4ede\n",
      "Ligand ligand_24.sdf uploaded: 1c2f8354-10c7-4521-9fcf-028751568ecd\n",
      "Ligand ligand_23.sdf uploaded: f718a3f7-b3c8-4bae-9cd0-dda4e1a52ab9\n",
      "Completed ligand_15: 200\n",
      "Ligand ligand_25.sdf uploaded: 06249ce7-9c83-4669-a4e6-62a54b6ea6dd\n",
      "Completed ligand_17: 200\n",
      "Completed ligand_16: 200\n",
      "Ligand ligand_27.sdf uploaded: 39ecdfba-3410-4e0d-8c71-238e03f92a8b\n",
      "Ligand ligand_26.sdf uploaded: 6e449de8-fc19-45b6-b088-c90ff145fd52\n",
      "Completed ligand_18: 200\n",
      "Ligand ligand_28.sdf uploaded: 46364e5b-13da-4bb9-af99-f7e62aa7e3be\n",
      "Completed ligand_20: 200\n",
      "Completed ligand_19: 200\n",
      "Ligand ligand_29.sdf uploaded: 28ee75e8-7660-4646-959b-ed14e552564d\n",
      "Ligand ligand_3.sdf uploaded: 956adac6-de88-4105-91bd-50ab3b2ef854\n",
      "Completed ligand_21: 200\n",
      "Ligand ligand_30.sdf uploaded: 5ca23156-40c7-40a1-943b-6c8e78c3708d\n",
      "Completed ligand_22: 200\n",
      "Completed ligand_23: 200\n",
      "Ligand ligand_31.sdf uploaded: a6fb5c52-bb90-4b1b-8e2f-863be4cf28b4\n",
      "Ligand ligand_32.sdf uploaded: cef83b9c-d177-4ac6-8fea-1240fad92985\n",
      "Completed ligand_24: 200\n",
      "Ligand ligand_33.sdf uploaded: 8957fa79-5e97-4650-b76e-5d35b12dffb3\n",
      "Completed ligand_25: 200\n",
      "Completed ligand_26: 200\n",
      "Ligand ligand_34.sdf uploaded: 3da8479b-7a08-433d-84b4-99f03b740339\n",
      "Ligand ligand_35.sdf uploaded: 4a3f4bd5-4c83-4d5a-85d5-8e35a68c6237\n",
      "Completed ligand_27: 200\n",
      "Ligand ligand_4.sdf uploaded: 028c04f2-7d56-4bed-b54a-8056bce465de\n",
      "Completed ligand_28: 200\n",
      "Completed ligand_29: 200\n",
      "Ligand ligand_5.sdf uploaded: 1fd1d097-d21d-4544-bbba-796b59fbbd58\n",
      "Ligand ligand_6.sdf uploaded: b769ab64-d94f-44da-b12c-a35c6320f0ba\n",
      "Completed ligand_30: 200\n",
      "Ligand ligand_7.sdf uploaded: 6d9aef4e-f549-43c4-8aab-df06120ba01a\n",
      "Completed ligand_31: 200\n",
      "Completed ligand_32: 200\n",
      "Ligand ligand_8.sdf uploaded: b158eae3-ceb5-40d0-b592-7cf5c383efb9\n",
      "Ligand ligand_9.sdf uploaded: 2d19d6ff-51e6-4a44-93bf-118809dbbf03\n",
      "Completed ligand_33: 200\n",
      "Completed ligand_34: 200\n",
      "Completed ligand_35: 200\n"
     ]
    }
   ],
   "source": [
    "#run diffdock\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---- CONFIG ----\n",
    "input_dir = \"sdf_output\"\n",
    "output_dir = \"results_output\"\n",
    "receptor_path = \"receptor_clean.pdb\"\n",
    "\n",
    "url = \"https://health.api.nvidia.com/v1/biology/mit/diffdock\"\n",
    "header_auth = \"Bearer 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\"\n",
    "\n",
    "# ---- ASSET UPLOAD FUNCTION ----\n",
    "def _upload_asset(input_data):\n",
    "    assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\"\n",
    "    headers = {\n",
    "        \"Authorization\": header_auth,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"accept\": \"application/json\",\n",
    "    }\n",
    "    s3_headers = {\n",
    "        \"x-amz-meta-nvcf-asset-description\": \"diffdock-file\",\n",
    "        \"content-type\": \"text/plain\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"contentType\": \"text/plain\",\n",
    "        \"description\": \"diffdock-file\"\n",
    "    }\n",
    "\n",
    "    for attempt in range(5):  # retry up to 5 times\n",
    "        try:\n",
    "            response = requests.post(assets_url, headers=headers, json=payload, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            asset_url = response.json()[\"uploadUrl\"]\n",
    "            asset_id = response.json()[\"assetId\"]\n",
    "\n",
    "            response = requests.put(asset_url, data=input_data, headers=s3_headers, timeout=300)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            return asset_id\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                wait = 2 ** attempt + random.uniform(0, 1)\n",
    "                print(f\"[WARN] Rate limited. Retrying after {wait:.2f}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise\n",
    "    raise RuntimeError(\"Failed to upload asset after multiple attempts\")\n",
    "\n",
    "# ---- UPLOAD PROTEIN ONCE ----\n",
    "with open(receptor_path, \"rb\") as f:\n",
    "    protein_id = _upload_asset(f.read())\n",
    "print(f\"Protein uploaded: {protein_id}\")\n",
    "\n",
    "# ---- PROCESS ONE LIGAND ----\n",
    "def process_ligand(idx, sdf_file):\n",
    "    try:\n",
    "        ligand_path = os.path.join(input_dir, sdf_file)\n",
    "        out_folder = os.path.join(output_dir, f\"ligand_{idx}\")\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "        with open(ligand_path, \"rb\") as f:\n",
    "            ligand_id = _upload_asset(f.read())\n",
    "\n",
    "        print(f\"Ligand {sdf_file} uploaded: {ligand_id}\")\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"NVCF-INPUT-ASSET-REFERENCES\": f\"{protein_id},{ligand_id}\",\n",
    "            \"Authorization\": header_auth\n",
    "        }\n",
    "\n",
    "        payload = {\n",
    "            \"ligand\": ligand_id,\n",
    "            \"ligand_file_type\": \"sdf\",\n",
    "            \"protein\": protein_id,\n",
    "            \"num_poses\": 20,\n",
    "            \"time_divisions\": 20,\n",
    "            \"steps\": 18,\n",
    "            \"save_trajectory\": True,\n",
    "            \"is_staged\": True\n",
    "        }\n",
    "\n",
    "        for attempt in range(5):  # Retry logic for rate-limited inference\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            if response.status_code != 429:\n",
    "                break\n",
    "            wait = 2 ** attempt + random.uniform(0, 1)\n",
    "            print(f\"[WARN] Inference rate-limited. Retrying after {wait:.2f}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        with open(os.path.join(out_folder, \"response_status.txt\"), \"w\") as f:\n",
    "            f.write(str(response))\n",
    "\n",
    "        with open(os.path.join(out_folder, \"request_url.txt\"), \"w\") as f:\n",
    "            f.write(url)\n",
    "\n",
    "        with open(os.path.join(out_folder, \"response_text.txt\"), \"w\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "        print(f\"Completed ligand_{idx}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ligand {idx} failed: {e}\")\n",
    "\n",
    "# ---- MULTITHREADING EXECUTION ----\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "sdf_files = [f for f in os.listdir(input_dir) if f.endswith(\".sdf\")]\n",
    "\n",
    "# Reduce concurrency to avoid 429 errors\n",
    "max_workers = min(3, len(sdf_files))  # Try 2–3 threads instead of 8\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_ligand, idx, sdf_file) for idx, sdf_file in enumerate(sdf_files)]\n",
    "    for future in as_completed(futures):\n",
    "        pass  # Ensures we wait for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "base_path = r\"results_output\"\n",
    "ligand_confidences = []\n",
    "\n",
    "\n",
    "for i in range(37):\n",
    "    ligand_folder = os.path.join(base_path, f\"ligand_{i}\")\n",
    "    input_file = os.path.join(ligand_folder, \"response_text.txt\")\n",
    "    output_folder = os.path.join(ligand_folder, \"diffdock_actual_outcome\")\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Missing file in {ligand_folder}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Write PDB files\n",
    "    for j, pose in enumerate(data.get(\"trajectory\", []), start=1):\n",
    "        with open(os.path.join(output_folder, f\"pose_{j}.pdb\"), \"w\") as pdb_file:\n",
    "            pdb_file.write(pose)\n",
    "\n",
    "    # Write SDF files\n",
    "    for j, sdf in enumerate(data.get(\"ligand_positions\", []), start=1):\n",
    "        with open(os.path.join(output_folder, f\"ligand_pose_{j}.sdf\"), \"w\") as sdf_file:\n",
    "            sdf_file.write(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to:\n",
      "results_output\\..\\Lipinski_before.txt\n"
     ]
    }
   ],
   "source": [
    "#reformatting the output, ranking them based on the confidence score, and extracting their smiles string.\n",
    "\n",
    "\n",
    "\n",
    "# --- Load SMILES from diffdock_before.txt ---\n",
    "smiles_path = os.path.join(base_path, \"..\\diffdock_before.txt\")\n",
    "with open(smiles_path, \"r\") as f:\n",
    "    smiles_list = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# --- Step 1: Collect confidence scores ---\n",
    "for i in range(len(smiles_list)):  # only process ligands with SMILES\n",
    "    ligand_folder = os.path.join(base_path, f\"ligand_{i}\")\n",
    "    input_file = os.path.join(ligand_folder, \"response_text.txt\")\n",
    "    output_folder = os.path.join(ligand_folder, \"diffdock_actual_outcome\")\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Write confidence scores\n",
    "    confidences = data.get(\"position_confidence\", [])\n",
    "    with open(os.path.join(output_folder, \"pose_confidences.txt\"), \"w\") as out_file:\n",
    "        out_file.write(\"Rank \\t Pose Confidence\\n\\n\")\n",
    "        for j, conf in enumerate(confidences, start=1):\n",
    "            out_file.write(f\"{j} \\t {conf}\\n\")\n",
    "\n",
    "    valid_confidences = [c for c in confidences if c is not None]\n",
    "    if valid_confidences:\n",
    "        highest = max(valid_confidences)\n",
    "        ligand_confidences.append((i, highest))\n",
    "\n",
    "# --- Step 2: Sort by confidence score ---\n",
    "ligand_confidences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# --- Step 3: Write output with ranking ---\n",
    "output_file = os.path.join(base_path, \"..\\Lipinski_before.txt\")\n",
    "with open(output_file, \"w\") as out:\n",
    "    out.write(\"Rank\\tLigand Number\\tSMILES\\tConfidence Score\\n\")\n",
    "    for rank, (ligand_num, confidence) in enumerate(ligand_confidences, start=1):\n",
    "        try:\n",
    "            smiles = smiles_list[ligand_num]\n",
    "        except IndexError:\n",
    "            smiles = \"ERROR: No SMILES available\"\n",
    "        out.write(f\"{rank}\\t{ligand_num}\\t{smiles}\\t{confidence:.4f}\\n\")\n",
    "\n",
    "print(\"Output written to:\")\n",
    "print(output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aaded9",
   "metadata": {},
   "source": [
    "# \"Lipinski's Rule of 5\" checking\n",
    "input: Lipinski_before.txt\n",
    "output:Lipinski_after.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 35 molecules that passed Lipinski’s rule to `Lipinski_after.txt`\n"
     ]
    }
   ],
   "source": [
    "#\"Lipinski's Rule of 5\" checking\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "def lipinski_violations(mol):\n",
    "    \"\"\"Return the count of Lipinski rule violations.\"\"\"\n",
    "    mw   = Descriptors.MolWt(mol)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    hbd  = Descriptors.NumHDonors(mol)\n",
    "    hba  = Descriptors.NumHAcceptors(mol)\n",
    "\n",
    "    violations = 0\n",
    "    if mw   >= 500: violations += 1\n",
    "    if logp >= 5:   violations += 1\n",
    "    if hbd  >= 5:   violations += 1\n",
    "    if hba  >= 10:  violations += 1\n",
    "\n",
    "    return violations\n",
    "\n",
    "def filter_lipinski(input_path='Lipinski_before.txt', output_path='Lipinski_after.txt'):\n",
    "    passed = []\n",
    "\n",
    "    with open(input_path, 'r') as infile:\n",
    "        header = infile.readline()  # skip header\n",
    "        for line in infile:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 4:\n",
    "                continue  # skip malformed lines\n",
    "\n",
    "            rank, ligand_num, smi, confidence = parts\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                continue\n",
    "\n",
    "            if lipinski_violations(mol) <= 1:\n",
    "                passed.append((rank, ligand_num, smi, confidence))\n",
    "\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        outfile.write(\"Rank\\tLigand Number\\tSMILES\\tConfidence Score\\n\")\n",
    "        for rank, ligand_num, smi, confidence in passed:\n",
    "            outfile.write(f\"{rank}\\t{ligand_num}\\t{smi}\\t{confidence}\\n\")\n",
    "\n",
    "    print(f\"Saved {len(passed)} molecules that passed Lipinski’s rule to `{output_path}`\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filter_lipinski()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a977e",
   "metadata": {},
   "source": [
    "Check binding pocket manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed17e3",
   "metadata": {},
   "source": [
    "# TxGemma Toxicity Predictor\n",
    "input: Lipinski_after.txt\n",
    "output: Lipinski_after_toxicity_checked.txt\n",
    "Need: HF token and Gemini API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8ea6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "genai.configure(api_key=\"AIzaSyC88Lstivmi4JrPb4znZtmU0l3WqG-DJLY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da7ae35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tdc_prompts_filepath = hf_hub_download(\n",
    "    repo_id=\"google/txgemma-27b-predict\",\n",
    "    filename=\"tdc_prompts.json\",\n",
    ")\n",
    "\n",
    "with open(tdc_prompts_filepath, \"r\") as f:\n",
    "    tdc_prompts_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18d8e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: Answer the following question about drug properties.\n",
      "Context: Humans are exposed to a variety of chemicals through food, household products, and medicines, some of which can be toxic, leading to over 30% of promising pharmaceuticals failing in human trials due to toxicity. Toxic drugs can be identified from clinical trials that failed due to toxicity, while non-toxic drugs can be identified from FDA approval status or from clinical trials that report no toxicity.\n",
      "Question: Given a drug SMILES string, predict whether it\n",
      "(A) is not toxic (B) is toxic\n",
      "Drug SMILES: {Drug SMILES}\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "dataset = \"ClinTox\"  #@param [\"AMES\", \"ClinTox\"]\n",
    "\n",
    "# now use it:\n",
    "prompt = tdc_prompts_json[dataset]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "PREDICT_VARIANT = \"9b-predict\"  # @param [\"2b-predict\", \"9b-predict\", \"27b-predict\"]\n",
    "CHAT_VARIANT = \"9b-chat\" # @param [\"9b-chat\", \"27b-chat\"]\n",
    "USE_CHAT = True # @param {type: \"boolean\"}\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "predict_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{PREDICT_VARIANT}\")\n",
    "predict_model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"google/txgemma-{PREDICT_VARIANT}\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "if USE_CHAT:\n",
    "    chat_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{CHAT_VARIANT}\")\n",
    "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "        f\"google/txgemma-{CHAT_VARIANT}\",\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95fbac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhaol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction model response: Instructions: Answer the following question about drug properties.\n",
      "Context: Mutagenicity means the ability of a drug to induce genetic alterations. Drugs that can cause damage to the DNA can result in cell death or other severe adverse effects. Nowadays, the most widely used assay for testing the mutagenicity of compounds is the Ames experiment which was invented by a professor named Ames. The Ames test is a short-term bacterial reverse mutation assay detecting a large number of compounds which can induce genetic damage and frameshift mutations.\n",
      "Question: Given a drug SMILES string, predict whether it\n",
      "(A) is not mutagenic (B) is mutagenic\n",
      "Drug SMILES: C[C@@H]1Cc2c([nH]c3ccccc23)[C@H](N1CC(F)(F)F)c1c(F)ccc(NCCNCCCF)c1F\n",
      "Answer:B)\n",
      "Chat model response: Instructions: Answer the following question about drug properties.\n",
      "Context: Mutagenicity means the ability of a drug to induce genetic alterations. Drugs that can cause damage to the DNA can result in cell death or other severe adverse effects. Nowadays, the most widely used assay for testing the mutagenicity of compounds is the Ames experiment which was invented by a professor named Ames. The Ames test is a short-term bacterial reverse mutation assay detecting a large number of compounds which can induce genetic damage and frameshift mutations.\n",
      "Question: Given a drug SMILES string, predict whether it\n",
      "(A) is not mutagenic (B) is mutagenic\n",
      "Drug SMILES: C[C@@H]1Cc2c([nH]c3ccccc23)[C@H](N1CC(F)(F)F)c1c(F)ccc(NCCNCCCF)c1F\n",
      "Answer: (A)\n"
     ]
    }
   ],
   "source": [
    "## Example task and input\n",
    "task_name = \"AMES\"\n",
    "smiles = \"{Drug SMILES}\"\n",
    "sequence = \"{Target amino acid sequence}\"\n",
    "drug_smiles = \"C[C@@H]1Cc2c([nH]c3ccccc23)[C@H](N1CC(F)(F)F)c1c(F)ccc(NCCNCCCF)c1F\"\n",
    "AA_sequence = \"MTAEKEKKRCSSERRKEKSRDAARCRRSKETEVFYELAHQLPIPHSISSHLDKASIMRLAISFLRTRKLLTSGCVAATETTDVDRLMDSWYLKPLGGFITVVTSDGDMIFLSENINKFMGLTQVELTGHSIFDFTHPCDHEEIRENLSLKAGMGKKGKELNTERDFFMRMKCTVTNRGRTVNLKSASWKVLHCTGHLKVCNGCPARVLCGFKEPPLTCVVMMCEPIPHPSNIDTPLDSKAFLSRHSMDMKFTYCDDRVTELMGYSPEDLLGRSAYDFYHALDSDNVTKSHQNLCTKGQAVSGQYRMLAKNGGYVWVETQGTVIYNSRNSQPQCIVCVNYVLSDVEEKSMIFSMDQTESLFKPHNLNSFFSPSKRSLGSDQSEALFTKLKEEPEDLTQLAPTPGDTIISLDFGQPQYEEHPMYSKVSSVAPPVSHSIHDGHKASYAGDMPKMAATFSVPQAPPPSSATPSLSSCSTPSSPGDYYTPVDSDLKVELTEKLFSLDTQETKASCNQENDLSDLDLETLAPYIPMDGEDFQLNPICQEEPASEIGGLVTNQQSFSNITSLFQPLGSSSAAHFQPNMSSGGDKKSISGGSVGSWPSIPCSRGPMQMPPYHDPASTPLSSMGGRQNLQWPPDPPLPSKAGMMDPLAAKRSCQTMPANRMPLYLQRPVENFVQNYRDMSPARLALTNGFKRSFTQMTMGESPPTKSQQTLWKRLRNESCAVMDRKSLSTSALSDKGMAHNRGMDHQHRKTQYSGNQTGQAAKCYREQCCNYREFSMQPSSKMDGIASRLIGPSFETYSLPELTRYDCEVNVPLQGNLHLLQGSDLLRALDQST\"\n",
    "TDC_PROMPT = tdc_prompts_json[task_name].replace(smiles, drug_smiles).replace(sequence, AA_sequence)\n",
    "\n",
    "def txgemma_predict(prompt):\n",
    "    input_ids = predict_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = predict_model.generate(**input_ids, max_new_tokens=8)\n",
    "    return predict_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def txgemma_chat(prompt):\n",
    "    input_ids = chat_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = chat_model.generate(**input_ids, max_new_tokens=32)\n",
    "    return chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prediction model response: {txgemma_predict(TDC_PROMPT)}\")\n",
    "if USE_CHAT: print(f\"Chat model response: {txgemma_chat(TDC_PROMPT)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c552efe",
   "metadata": {},
   "source": [
    "# Tool to allow our Agentic-Tx to ask TxGemma therapeutically relevant questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e735b8",
   "metadata": {},
   "source": [
    "Making a tool for our agent to use: a chat interface for our llama-based Agentic-Tx and TxGemma-Chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ffa2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow us to extract content from inside of ticks\n",
    "def extract_prompt(text, word):\n",
    "    code_block_pattern = rf\"```{word}(.*?)```\"\n",
    "    code_blocks = re.findall(code_block_pattern, text, re.DOTALL)\n",
    "    extracted_code = \"\\n\".join(code_blocks).strip()\n",
    "    return extracted_code\n",
    "\n",
    "# This class will allow us to inferface with TxGemma\n",
    "class TxGemmaChatTool:\n",
    "    def __init__(self):\n",
    "      self.tool_name = \"Chat Tool\"\n",
    "\n",
    "    def use_tool(self, question):\n",
    "        # Here, we are submitting a question to TxGemma\n",
    "        response = txgemma_chat(question)\n",
    "        return response\n",
    "\n",
    "    def tool_is_used(self, query):\n",
    "        # This just checks to see if the tool call was evoked\n",
    "        return \"```TxGemmaChat\" in query\n",
    "\n",
    "    def process_query(self, query):\n",
    "        # Here, we clean to query to remove the tool call\n",
    "        return extract_prompt(query, word=\"TxGemmaChat\")\n",
    "\n",
    "    def instructions(self):\n",
    "        # Here, we are **very** descriptively explaining how the tool works to the agent\n",
    "        # This will be useful later on\n",
    "        return (\n",
    "            \"=== Therapeutic Chat Tool Instructions ===\\n\"\n",
    "            \"### What This Tool Does\\n\"\n",
    "            \"The Therapeutic Chat Tool allows you to chat with a knowledgeable large language model named TxGemma trained on many therapeutics datasets.\"\n",
    "            \"### When and Why You Should Use It\\n\"\n",
    "            \"- If you have therapeutics related questions that you would benefit from asking TxGemma from.\\n\"\n",
    "            \"### How to Use It\\n\"\n",
    "            \"Format your query with triple backticks (```), and start with `TxGemmaChat`. Then on a new line:\\n\"\n",
    "            \"1) **Any question you would like to ask**\\n\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"```TxGemmaChat\\n\"\n",
    "            \"What is a common drug used to treat ovarian cancer?\\n\"\n",
    "            \"```\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd742ab9",
   "metadata": {},
   "source": [
    "# Making a TxGemma prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3772350",
   "metadata": {},
   "source": [
    "AMES Mutagenicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fe4122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will allow us to predict binding affinity using TxGemma\n",
    "class Mutagenicity:\n",
    "    def __init__(self):\n",
    "      self.tool_name = \"AMES Mutagenicity Prediction\"\n",
    "\n",
    "    def use_tool(self, smiles_string):\n",
    "        # Here, we are submitting the smiles to TxGemma, and returning the response\n",
    "        prediction = txgemma_predict(tdc_prompts_json[\"AMES\"].replace(\"{Drug SMILES}\", smiles_string))\n",
    "        if \"(A)\" in prediction:   prediction = f\"{smiles_string} is not mutagenic!\"\n",
    "        elif \"(B)\" in prediction: prediction = f\"{smiles_string} is mutagenic!\"\n",
    "        return prediction\n",
    "\n",
    "    def tool_is_used(self, query):\n",
    "        # This just checks to see if the tool call was evoked\n",
    "        return \"```MutagenicityPred\" in query\n",
    "\n",
    "    def process_query(self, query):\n",
    "        # Here, we clean to query to remove the tool call\n",
    "        return extract_prompt(query, word=\"MutagenicityPred\")\n",
    "\n",
    "    def instructions(self):\n",
    "        # Here, we are explaining how the tool works to the agent\n",
    "        return (\n",
    "        \"=== AMES Mutagenicity Prediction Instructions ===\\n\"\n",
    "            \"The AMES Mutagenicity Prediction Tool is designed to predict potential for mutagenicity for humans in clinicial trials.\\n\"\n",
    "            \"You can test the mutagenicity of different SMILES strings as they might affect humans.\\n\"\n",
    "            \"To properly use this tool, follow the format outlined below:\\n\"\n",
    "            \"1. **Form a AMES Mutagenicity Prediction query**:\\n\"\n",
    "            \"```MutagenicityPred\\n\\n```\\n\"\n",
    "            \"Example: ```MutagenicityPred\\nCN(C)C(=N)N=C(N)N\\n```\\n\"\n",
    "            \"- Replace `` with an exact smiles string. \"\n",
    "            \"A result will be returned to you describing the AMES Mutagenicity Prediction.\\n\"\n",
    "            \"**Important Formatting Details**:\\n\"\n",
    "            \"- Use `MutagenicityPred` as the exact keyword to begin your query.\\n\"\n",
    "            \"- Place your text after `MutagenicityPred` on a new line.\\n\"\n",
    "            \"- Enclose the entire query using three backticks (```), as shown in the example above.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85948b73",
   "metadata": {},
   "source": [
    "ClinTox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "940d3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinTox:\n",
    "    def __init__(self):\n",
    "      self.tool_name = \"Clinical Toxicology Prediction\"\n",
    "\n",
    "    def use_tool(self, smiles_string):\n",
    "        # Here, we are submitting the smiles to TxGemma, and returning the response\n",
    "        prediction = txgemma_predict(tdc_prompts_json[\"ClinTox\"].replace(\"{Drug SMILES}\", smiles_string))\n",
    "        if \"(A)\" in prediction:   prediction = f\"{smiles_string} is predicted to be toxic!\"\n",
    "        elif \"(B)\" in prediction: prediction = f\"{smiles_string} is predicted to be not toxic!\"\n",
    "        return prediction\n",
    "\n",
    "    def tool_is_used(self, query):\n",
    "        # This just checks to see if the tool call was evoked\n",
    "        return \"```ClinToxPred\" in query\n",
    "\n",
    "    def process_query(self, query):\n",
    "        # Here, we clean to query to remove the tool call\n",
    "        return extract_prompt(query, word=\"ClinToxPred\")\n",
    "\n",
    "    def instructions(self):\n",
    "        # Here, we are explaining how the tool works to the agent\n",
    "        return (\n",
    "        \"=== Clinical Toxicology Prediction Instructions ===\\n\"\n",
    "            \"The Clinical Toxicology Prediction Tool is designed to predict potential for toxicology for humans in clinicial trials.\\n\"\n",
    "            \"You can test the toxicology of different SMILES strings as they might affect humans.\\n\"\n",
    "            \"To properly use this tool, follow the format outlined below:\\n\"\n",
    "            \"1. **Form a Clinical Toxicology Prediction query**:\\n\"\n",
    "            \"```ClinToxPred\\n\\n```\\n\"\n",
    "            \"Example: ```ClinToxPred\\nCN(C)C(=N)N=C(N)N\\n```\\n\"\n",
    "            \"- Replace `` with an exact smiles string. \"\n",
    "            \"A result will be returned to you describing the Clinical Toxicology Prediction.\\n\"\n",
    "            \"**Important Formatting Details**:\\n\"\n",
    "            \"- Use `ClinToxPred` as the exact keyword to begin your query.\\n\"\n",
    "            \"- Place your text after `ClinToxPred` on a new line.\\n\"\n",
    "            \"- Enclose the entire query using three backticks (```), as shown in the example above.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012afc7b",
   "metadata": {},
   "source": [
    "# PubMed search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdfb1df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --quiet biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f02bd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Medline, Entrez\n",
    "\n",
    "# This class will allow us to interface with PubMed\n",
    "class PubMedSearch:\n",
    "    def __init__(self):\n",
    "      self.tool_name = \"PubMed Search\"\n",
    "\n",
    "    def tool_is_used(self, query: str):\n",
    "        # This just checks to see if the tool call was evoked\n",
    "        return \"```PubMedSearch\" in query\n",
    "\n",
    "    def process_query(self, query: str):\n",
    "        # Here, we clean to query to remove the tool call\n",
    "        search_text = extract_prompt(query, word=\"PubMedSearch\")\n",
    "        return search_text.strip()\n",
    "\n",
    "    def use_tool(self, search_text):\n",
    "        # Here, we are searching through PubMed and returning relevant articles\n",
    "        pmids = list()\n",
    "        handle = Entrez.esearch(db=\"pubmed\", sort=\"relevance\", term=search_text, retmax=3)\n",
    "        record = Entrez.read(handle)\n",
    "        pmids = record.get(\"IdList\", [])\n",
    "        handle.close()\n",
    "\n",
    "        if not pmids:\n",
    "            return f\"No PubMed articles found for '{search_text}' Please try a simpler search query.\"\n",
    "\n",
    "        fetch_handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pmids), rettype=\"medline\", retmode=\"text\")\n",
    "        records = list(Medline.parse(fetch_handle))\n",
    "        fetch_handle.close()\n",
    "\n",
    "        result_str = f\"=== PubMed Search Results for: '{search_text}' ===\\n\"\n",
    "        for i, record in enumerate(records, start=1):\n",
    "            pmid = record.get(\"PMID\", \"N/A\")\n",
    "            title = record.get(\"TI\", \"No title available\")\n",
    "            abstract = record.get(\"AB\", \"No abstract available\")\n",
    "            journal = record.get(\"JT\", \"No journal info\")\n",
    "            pub_date = record.get(\"DP\", \"No date info\")\n",
    "            authors = record.get(\"AU\", [])\n",
    "            authors_str = \", \".join(authors[:3])\n",
    "            result_str += (\n",
    "                f\"\\n--- Article #{i} ---\\n\"\n",
    "                f\"PMID: {pmid}\\n\"\n",
    "                f\"Title: {title}\\n\"\n",
    "                f\"Authors: {authors_str}\\n\"\n",
    "                f\"Journal: {journal}\\n\"\n",
    "                f\"Publication Date: {pub_date}\\n\"\n",
    "                f\"Abstract: {abstract}\\n\")\n",
    "        return f\"Query: {search_text}\\nResults: {result_str}\"\n",
    "\n",
    "    def instructions(self):\n",
    "        # Here, we are explaining how the tool works to the agent\n",
    "        return (\n",
    "            f\"{'@' * 10}\\n@@@ PubMed Search Tool Instructions @@@\\n\\n\"\n",
    "            \"### What This Tool Does\\n\"\n",
    "            \"The PubMed Search Tool queries the NCBI Entrez API (PubMed) for a given search phrase, \"\n",
    "            \"and retrieves metadata for a few of the top articles (PMID, title, authors, journal, date, abstract).\\n\\n\"\n",
    "            \"### When / Why You Should Use It\\n\"\n",
    "            \"- To find **scientific literature** references on a specific biomedical topic.\\n\"\n",
    "            \"- To retrieve **abstracts, titles, authors**, and other metadata.\\n\\n\"\n",
    "            \"### Query Format\\n\"\n",
    "            \"Wrap your request with triple backticks, starting with `PubMedSearch`. For example:\\n\\n\"\n",
    "            \"```PubMedSearch\\ncancer immunotherapy\\n```\\n\\n\"\n",
    "            \"### Example\\n\"\n",
    "            \"```PubMedSearch\\nmachine learning in drug discovery\\n```\\n\"\n",
    "            \"- This will search PubMed for articles related to 'machine learning in drug discovery', \"\n",
    "            \"fetch up to 3 PMIDs, and return their titles, abstracts, etc.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97cafd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhaol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Bio\\Entrez\\__init__.py:734: UserWarning: \n",
      "            Email address is not specified.\n",
      "\n",
      "            To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "            email address with each request.  As an example, if your email address\n",
      "            is A.N.Other@example.com, you can specify it as follows:\n",
      "               from Bio import Entrez\n",
      "               Entrez.email = 'A.N.Other@example.com'\n",
      "            In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "            a user at the email address provided before blocking access to the\n",
      "            E-utilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Is aspirin toxic?\n",
      "Results: === PubMed Search Results for: 'Is aspirin toxic?' ===\n",
      "\n",
      "--- Article #1 ---\n",
      "PMID: 39092921\n",
      "Title: Pharmacokinetics of aspirin: evaluating shortcomings in the literature.\n",
      "Authors: Visagie JL, Aruwajoye GS, van der Sluis R\n",
      "Journal: Expert opinion on drug metabolism & toxicology\n",
      "Publication Date: 2024 Aug\n",
      "Abstract: INTRODUCTION: Aspirin is known for its therapeutic benefits in preventing strokes and relieving pain. However, it is toxic to some individuals, and the biological mechanisms causing toxicity are unknown. Limited literature is available on the role of glycine conjugation as the principal pathway in aspirin detoxification. Previous studies have quantified this two-step enzyme reaction as a singular enzymatic process. Consequently, the individual contributions of these enzymes to the kinetics remain unclear. AREAS COVERED: This review summarized the available information on the pharmacokinetics and detoxification of aspirin by the glycine conjugation pathway. Literature searches were conducted using Google Scholar and the academic journal databases accessible through the North-West University Library. Furthermore, the factors affecting interindividual variation in aspirin metabolism and what is known regarding aspirin toxicity were discussed. EXPERT OPINION: The greatest drawback in understanding the pharmacokinetics of aspirin is the limited information available on the substrate preference of the xenobiotic ligase (ACSM) responsible for activating salicylate to salicyl-CoA. Furthermore, previous pharmacokinetic studies did not consider the contribution of other substrates from the diet or genetic variants, to the detoxification rate of glycine conjugation. Impaired glycine conjugation might contribute to adverse health effects seen in Reye's syndrome and cancer.\n",
      "\n",
      "--- Article #2 ---\n",
      "PMID: 24361050\n",
      "Title: An aspirin a day.\n",
      "Authors: Majerus PW\n",
      "Journal: Advances in biological regulation\n",
      "Publication Date: 2014 Jan\n",
      "Abstract: The title of this article is also its punch line. The thesis that I will prove is that every adult, with a few exceptions, should take one 325 mg aspirin tablet each day. The drug is extraordinary and is beneficial in myriad ways. In this dosage the toxicity of the treatment is minimal. Since the drug is sold \"over the counter\", not requiring prescription, it is cheap and its benefits are easily underestimated. I do not use extensive reference citations; but just tell the story of aspirin.\n",
      "\n",
      "--- Article #3 ---\n",
      "PMID: 10174319\n",
      "Title: Is aspirin underused in myocardial infarction?\n",
      "Authors: Arnau JM, Agusti A\n",
      "Journal: PharmacoEconomics\n",
      "Publication Date: 1997 Nov\n",
      "Abstract: This article reviews the relevant published literature in order to assess whether aspirin (acetylsalicylic acid; ASA) is underused in myocardial infarction (MI), taking into account: (i) the evidence of efficacy and safety from clinical trials; (ii) authoritative recommendations about its use; and (iii) published drug-utilisation studies. The use of low-dosage aspirin in the acute phase of MI, and as secondary prevention, should be recommended to all patients who do not have contraindications to the drug. This is a solid evidence-based recommendation with potential benefits that are, at least, similar to those obtained with other standard treatments. As this treatment is well tolerated and inexpensive, it is also assumed that net savings can be achieved. No conventionally used prophylactic aspirin regimen seems to be free from the risk of serious gastrointestinal toxicity. This is especially important in primary prevention, in which the benefits are small; there is, as yet, no clear evidence that aspirin is indicated for routine use in patients at low risk of occlusive vascular events. We have identified 21 published drug-utilisation studies, and the potential underuse of aspirin in MI was not properly assessed in most of them. In these studies, fairly high aspirin prescription rates were usually documented. However, it seems clear that there is room for improvement, and that a significant proportion of patients who could have benefited from aspirin did not receive it or received less well-studied and more costly drugs. The prescription rates for other drugs with proven efficacy have been lower, and the potential underuse greater, than those documented for aspirin.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pubmed_tool = PubMedSearch()\n",
    "search_results = pubmed_tool.use_tool(\"Is aspirin toxic?\")\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e0361",
   "metadata": {},
   "source": [
    "# Wrapping it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0094300",
   "metadata": {},
   "source": [
    "### Creating a tool manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38105be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tool manager will hold all of the tools, and provide an interface for the agent\n",
    "class ToolManager:\n",
    "    def __init__(self, toolset):\n",
    "        self.toolset = toolset\n",
    "\n",
    "    def tool_prompt(self):\n",
    "        # This will let the agent know what tools it has access to\n",
    "        tool_names = \", \".join([tool.tool_name for tool in self.toolset])\n",
    "        return f\"You have access to the following tools: {tool_names}\\n{self.tool_instructions()}. You can only use one tool at a time. These are the only tools you have access to nothing else.\"\n",
    "\n",
    "    def tool_instructions(self):\n",
    "        # This allows the agent to know how to use the tools\n",
    "        tool_instr = \"\\n\".join([tool.instructions() for tool in self.toolset])\n",
    "        return f\"The following is a set of instructions on how to use each tool.\\n{tool_instr}\"\n",
    "\n",
    "    def use_tool(self, query):\n",
    "        # This will iterate through all of the tools\n",
    "        # and find the correct tool that the agent requested\n",
    "        for tool in self.toolset:\n",
    "            if tool.tool_is_used(query):\n",
    "                # use the tool and return the output\n",
    "                return tool.use_tool(tool.process_query(query))\n",
    "        return f\"No tool match for search: {query}\"\n",
    "\n",
    "if USE_CHAT:\n",
    "    tools = ToolManager([TxGemmaChatTool(), Mutagenicity(), ClinTox(), PubMedSearch()])\n",
    "else:\n",
    "    tools = ToolManager([Mutagenicity(), ClinTox(), PubMedSearch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fb3db",
   "metadata": {},
   "source": [
    "### Creating a gemini inference tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b202b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_gemini(prompt, system_prompt, model_str):\n",
    "  # Check to see that our model string matches\n",
    "  if model_str == \"gemini-2.5-flash\":\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-2.5-flash-preview-05-20\", system_instruction=system_prompt)\n",
    "    response = model.generate_content(prompt)\n",
    "    answer = response.text\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccdc7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inference_llama(prompt: str, system_prompt: str, model_str: str) -> str:\n",
    "#     if model_str == \"llama-3.1-8B\":\n",
    "#         full_prompt = system_prompt.strip() + \"\\n\" + prompt.strip()\n",
    "#         outputs = pipe(full_prompt, return_full_text=False)\n",
    "#         return outputs[0][\"generated_text\"]\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported model_str: {model_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bf5d4",
   "metadata": {},
   "source": [
    "# Creating a therapeutics agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343eb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "# This class defines our Agentic-Tx, wrapping together all of our tools and the orchestrator\n",
    "class AgenticTx:\n",
    "    def __init__(self, tool_manager, model_str, num_steps=5):\n",
    "        self.curr_steps = 0\n",
    "        self.num_steps = num_steps\n",
    "        self.model_str = model_str\n",
    "        self.tool_manager = tool_manager\n",
    "        self.thoughts = []\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_steps = 0\n",
    "        self.thoughts.clear()\n",
    "        self.actions.clear()\n",
    "        self.observations.clear()\n",
    "\n",
    "    def system_prompt(self, use_tools=True):\n",
    "        role_prompt = \"You are an expert therapeutic agent. You answer accurately and thoroughly.\"\n",
    "        prev_actions = f\"You can perform a maximum of {self.num_steps} actions. You have performed {self.curr_steps} and have {self.num_steps - self.curr_steps - 1} left.\"\n",
    "        tool_prompt = (\"You can use tools to solve problems and answer questions. \" + self.tool_manager.tool_prompt()) if use_tools else \"You cannot use any tools right now.\"\n",
    "        return f\"{role_prompt} {prev_actions} {tool_prompt}\"\n",
    "\n",
    "    def prior_information(self, query):\n",
    "        info_txt = f\"Question: {query}\\n\" if query else \"\"\n",
    "        for _i in range(self.curr_steps):\n",
    "            info_txt += f\"### Thought {_i + 1}: {self.thoughts[_i]}\\n\"\n",
    "            info_txt += f\"### Action {_i + 1}: {self.actions[_i]}\\n\"\n",
    "            info_txt += f\"### Observation {_i + 1}: {self.observations[_i]}\\n\\n\"\n",
    "            info_txt += \"@\" * 20\n",
    "        return info_txt\n",
    "\n",
    "    def step(self, question):\n",
    "        self.reset()\n",
    "        for _i in range(self.num_steps):\n",
    "            if self.curr_steps == self.num_steps - 1:\n",
    "                return inference_gemini(\n",
    "                    model_str=self.model_str,\n",
    "                    prompt=f\"{self.prior_information(question)}\\nYou must now provide an answer to this question {question}\",\n",
    "                    system_prompt=self.system_prompt(use_tools=False))\n",
    "            else:\n",
    "                thought = inference_gemini(\n",
    "                    model_str=self.model_str,\n",
    "                    prompt=f\"{self.prior_information(question)}\\nYou cannot currently use tools but you can think about the problem and what tools you want to use. This was the question, think about plans for how to use tools to answer this {question}. Let's think step by step (respond with only 1-2 sentences).\\nThought: \",\n",
    "                    system_prompt=self.system_prompt(use_tools=False))\n",
    "                action = inference_gemini(\n",
    "                    model_str=self.model_str,\n",
    "                    prompt=f\"{self.prior_information(question)}\\n{thought}\\nNow you must use tools to answer the following user query [{question}], closely following the tool instructions. Tool\",\n",
    "                    system_prompt=self.system_prompt(use_tools=True))\n",
    "                obs = self.tool_manager.use_tool(action)\n",
    "\n",
    "                print(\"Thought:\", thought)\n",
    "                print(\"Action:\", action)\n",
    "                print(\"Observation:\", obs)\n",
    "\n",
    "                self.thoughts.append(thought)\n",
    "                self.actions.append(action)\n",
    "                self.observations.append(obs)\n",
    "\n",
    "                self.curr_steps += 1\n",
    "\n",
    "\n",
    "# Instantiate your agent\n",
    "agentictx = AgenticTx(tool_manager=tools, model_str=\"gemini-2.5-flash\")\n",
    "\n",
    "# Process the SMILES list from file\n",
    "input_file = \"Lipinski_after.txt\"\n",
    "output_file = \"Lipinski_after_toxicity_checked.txt\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Input file '{input_file}' not found.\")\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    count = 0\n",
    "    for line in infile:\n",
    "        smiles = line.strip()\n",
    "        if not smiles or smiles.startswith(\"#\"):\n",
    "            continue\n",
    "\n",
    "        question = f\"Is this drug toxic {smiles}? If it is toxic, what are the properties that make it toxic? If it is toxic, can you suggest what functional groups I should rreplace the toxic sections with?\"\n",
    "        print(f\"\\nProcessing SMILES #{count + 1}: {smiles}\")\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = agentictx.step(question)\n",
    "                outfile.write(f\"SMILES: {smiles}\\nResponse: {response}\\n{'='*60}\\n\")\n",
    "                count += 1\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    wait_time = 30\n",
    "                    print(f\"Rate limit hit. Waiting {wait_time} seconds (Attempt {attempt+1}/{max_retries})...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    outfile.write(f\"SMILES: {smiles}\\nError: {e}\\n{'='*60}\\n\")\n",
    "                    break\n",
    "\n",
    "print(f\"\\nToxicity analysis completed. {count} SMILES processed. Results written to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380b222",
   "metadata": {},
   "source": [
    "# TxGemma PK Properties Predictor\n",
    "input: Lipinski_after.txt\n",
    "output: Lipinski_after_pk_checked.txt\n",
    "Need: HF token and Gemini API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d173701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "genai.configure(api_key=\"AIzaSyCrL36wexWM9S4lQ3rAA9VND13b6MGgH3g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c65b6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tdc_prompts_filepath = hf_hub_download(\n",
    "    repo_id=\"google/txgemma-27b-predict\",\n",
    "    filename=\"tdc_prompts.json\",\n",
    ")\n",
    "\n",
    "with open(tdc_prompts_filepath, \"r\", encoding='utf-8') as f:\n",
    "    tdc_prompts_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3060490d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instructions: Answer the following question about drug properties.\\nContext: Drug clearance is defined as the volume of plasma cleared of a drug over a specified time period and it measures the rate at which the active drug is removed from the body.\\nQuestion: Given a drug SMILES string, predict its normalized hepatocyte clearance from 000 to 1000, where 000 is minimum hepatocyte clearance and 1000 is maximum hepatocyte clearance.\\nDrug SMILES: {Drug SMILES}\\nAnswer:'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Clearance Hepatocyte AZ: Given a drug SMILES, predict the activity of hepatocyte clearance.\n",
    "tdc_prompts_json[\"Clearance_Hepatocyte_AZ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15461371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instructions: Answer the following question about drug properties.\\nContext: Drug clearance is defined as the volume of plasma cleared of a drug over a specified time period and it measures the rate at which the active drug is removed from the body.\\nQuestion: Given a drug SMILES string, predict its normalized microsome clearance activity from 000 to 1000, where 000 is minimum microsome clearance and 1000 is maximum microsome clearance.\\nDrug SMILES: {Drug SMILES}\\nAnswer:'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Clearance Microsome AZ: Given a drug SMILES, predict the activity of microsome clearance.\n",
    "tdc_prompts_json[\"Clearance_Microsome_AZ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20c25111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instructions: Answer the following question about drug properties.\\nContext: Half life of a drug is the duration for the concentration of the drug in the body to be reduced by half. It measures the duration of actions of a drug. \\nQuestion: Given a drug SMILES string, predict its normalized half life from 000 to 1000, where 000 is minimum half life and 1000 is maximum half life.\\nDrug SMILES: {Drug SMILES}\\nAnswer:'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Half Life Obach: Given a drug SMILES, predict the half life duration.\n",
    "tdc_prompts_json[\"Half_Life_Obach\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82a0f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Instructions: Answer the following question about drug properties.\\nContext: The volume of distribution at steady state (VDss) measures the degree of a drug's concentration in body tissue compared to concentration in blood. Higher VD indicates a higher distribution in the tissue and usually indicates the drug with high lipid solubility, low plasma protein binding rate.\\nQuestion: Given a drug SMILES string, predict its normalized volume of distribution from 000 to 1000, where 000 is minimum volume of distribution and 1000 is maximum volume of distribution.\\nDrug SMILES: {Drug SMILES}\\nAnswer:\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####VDss Lombardo: Given a drug SMILES, predict the volume of distributon.\n",
    "tdc_prompts_json[\"VDss_Lombardo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1edcf79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instructions: Answer the following question about drug properties.\\nContext: Oral bioavailability is defined as “the rate and extent to which the active ingredient or active moiety is absorbed from a drug product and becomes available at the site of action”.\\n\\n\\nQuestion: Given a drug SMILES string, predict whether it\\n(A) has oral bioavailability < 20% (B) has oral bioavailability ≥ 20%\\nDrug SMILES: {Drug SMILES}\\nAnswer:'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Bioavailability Ma : Given a drug SMILES, predict whether it is orally available.\n",
    "tdc_prompts_json[\"Bioavailability_Ma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "PREDICT_VARIANT = \"9b-predict\"  # @param [\"2b-predict\", \"9b-predict\", \"27b-predict\"]\n",
    "CHAT_VARIANT = \"9b-chat\" # @param [\"9b-chat\", \"27b-chat\"]\n",
    "USE_CHAT = True # @param {type: \"boolean\"}\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "predict_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{PREDICT_VARIANT}\")\n",
    "predict_model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"google/txgemma-{PREDICT_VARIANT}\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "if USE_CHAT:\n",
    "    chat_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{CHAT_VARIANT}\")\n",
    "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "        f\"google/txgemma-{CHAT_VARIANT}\",\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe20090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example task and input\n",
    "task_name = \"Half_Life_Obach\"\n",
    "smiles = \"{Drug SMILES}\"\n",
    "drug_smiles = \"C[C@@H]1Cc2c([nH]c3ccccc23)[C@H](N1CC(F)(F)F)c1c(F)ccc(NCCNCCCF)c1F\"\n",
    "\n",
    "TDC_PROMPT = tdc_prompts_json[task_name].replace(smiles, drug_smiles)\n",
    "\n",
    "def txgemma_predict(prompt):\n",
    "    input_ids = predict_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = predict_model.generate(**input_ids, max_new_tokens=8)\n",
    "    return predict_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def txgemma_chat(prompt):\n",
    "    input_ids = chat_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = chat_model.generate(**input_ids, max_new_tokens=32)\n",
    "    return chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prediction model response: {txgemma_predict(TDC_PROMPT)}\")\n",
    "if USE_CHAT: print(f\"Chat model response: {txgemma_chat(TDC_PROMPT)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462dfe27",
   "metadata": {},
   "source": [
    "# Tool to allow our Agentic-Tx to ask TxGemma therapeutically relevant questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97a1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow us to extract content from inside of ticks\n",
    "def extract_prompt(text, word):\n",
    "    code_block_pattern = rf\"```{word}(.*?)```\"\n",
    "    code_blocks = re.findall(code_block_pattern, text, re.DOTALL)\n",
    "    extracted_code = \"\\n\".join(code_blocks).strip()\n",
    "    return extracted_code\n",
    "\n",
    "# This class will allow us to inferface with TxGemma\n",
    "class TxGemmaChatTool:\n",
    "    def __init__(self):\n",
    "      self.tool_name = \"Chat Tool\"\n",
    "\n",
    "    def use_tool(self, question):\n",
    "        # Here, we are submitting a question to TxGemma\n",
    "        response = txgemma_chat(question)\n",
    "        return response\n",
    "\n",
    "    def tool_is_used(self, query):\n",
    "        # This just checks to see if the tool call was evoked\n",
    "        return \"```TxGemmaChat\" in query\n",
    "\n",
    "    def process_query(self, query):\n",
    "        # Here, we clean to query to remove the tool call\n",
    "        return extract_prompt(query, word=\"TxGemmaChat\")\n",
    "\n",
    "    def instructions(self):\n",
    "        # Here, we are **very** descriptively explaining how the tool works to the agent\n",
    "        # This will be useful later on\n",
    "        return (\n",
    "            \"=== Therapeutic Chat Tool Instructions ===\\n\"\n",
    "            \"### What This Tool Does\\n\"\n",
    "            \"The Therapeutic Chat Tool allows you to chat with a knowledgeable large language model named TxGemma trained on many therapeutics datasets.\"\n",
    "            \"### When and Why You Should Use It\\n\"\n",
    "            \"- If you have therapeutics related questions that you would benefit from asking TxGemma from.\\n\"\n",
    "            \"### How to Use It\\n\"\n",
    "            \"Format your query with triple backticks (```), and start with `TxGemmaChat`. Then on a new line:\\n\"\n",
    "            \"1) **Any question you would like to ask**\\n\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"```TxGemmaChat\\n\"\n",
    "            \"What is a common drug used to treat ovarian cancer?\\n\"\n",
    "            \"```\\n\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b179fc",
   "metadata": {},
   "source": [
    "# Making a TxGemma prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a5c38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bioavailability Ma\n",
    "class BioavailabilityPred:\n",
    "    def __init__(self):\n",
    "        self.tool_name = \"Oral Bioavailability Prediction\"\n",
    "\n",
    "    def use_tool(self, smiles_string):\n",
    "        # Assuming txgemma_predict and tdc_prompts_json have an entry for Bioavailability\n",
    "        prediction = txgemma_predict(tdc_prompts_json[\"Bioavailability_Ma\"].replace(\"{Drug SMILES}\", smiles_string))\n",
    "        if \"(A)\" in prediction:   prediction = f\"{smiles_string} is predicted to have oral bioavailability < 20%!\"\n",
    "        elif \"(B)\" in prediction: prediction = f\"{smiles_string} is predicted to have oral bioavailability ≥ 20%!\"\n",
    "        return prediction\n",
    "\n",
    "    def tool_is_used(self, query):\n",
    "        # Check for exact keyword in query\n",
    "        return \"```BioavailabilityPred\" in query\n",
    "\n",
    "    def process_query(self, query):\n",
    "        # Clean query to remove tool call block and extract prompt\n",
    "        return extract_prompt(query, word=\"BioavailabilityPred\")\n",
    "\n",
    "    def instructions(self):\n",
    "        return (\n",
    "            \"=== Oral Bioavailability Prediction Instructions ===\\n\"\n",
    "            \"This tool predicts whether a small molecule (given as SMILES) is orally bioavailable.\\n\\n\"\n",
    "            \"To use this tool, invoke it exactly like this:\\n\"\n",
    "            \"```BioavailabilityPred\\n\"\n",
    "            \"{Drug SMILES}\\n\"\n",
    "            \"```\\n\\n\"\n",
    "            \"• **Keyword**: `BioavailabilityPred` (must match exactly).\\n\"\n",
    "            \"• **Line 2**: the SMILES string of your ligand.\\n\\n\"\n",
    "            \"**Example:**\\n\"\n",
    "            \"```BioavailabilityPred\\n\"\n",
    "            \"CC(=O)Oc1ccccc1C(=O)O\\n\"\n",
    "            \"```\\n\"\n",
    "            \"This will return a prediction on oral bioavailability for that molecule.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "639e6b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COc1c(F)cccc1C1=C(c2ccc(O[C@H]3CCN(CCCF)C3)cc2)c2ccc(O)cc2CCC1 is predicted to have oral bioavailability < 20%!\n"
     ]
    }
   ],
   "source": [
    "bioavailPred = BioavailabilityPred()\n",
    "\n",
    "# Use only the SMILES string since BioavailabilityPred takes just that\n",
    "smiles = \"COc1c(F)cccc1C1=C(c2ccc(O[C@H]3CCN(CCCF)C3)cc2)c2ccc(O)cc2CCC1\"\n",
    "prediction_bioavail = bioavailPred.use_tool(smiles)\n",
    "print(prediction_bioavail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3902b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClearanceHepatocyteAZPred:\n",
    "#     def __init__(self):\n",
    "#         self.tool_name = \"Clearance Hepatocyte AZ Prediction\"\n",
    "\n",
    "#     def use_tool(self, smiles_string):\n",
    "#         # Assuming txgemma_predict and tdc_prompts_json have an entry for ClearanceHepatocyteAZ\n",
    "#         prediction = txgemma_predict(\n",
    "#             tdc_prompts_json[\"Clearance_Hepatocyte_AZ\"].replace(\"{Drug SMILES}\", smiles_string)\n",
    "#         )\n",
    "#         # Example expected output: \"Answer: Clearance value: 45.6\"\n",
    "#         match = re.search(r\"Answer:\\s*(?:Clearance value:\\s*)?([0-9]*\\.?[0-9]+)\", prediction)\n",
    "        \n",
    "#         if match:\n",
    "#             clearance_value = match.group(1)\n",
    "#             return f\"{smiles_string} is predicted to have hepatocyte clearance with a value of {clearance_value}(L/min).\"\n",
    "#         else:\n",
    "#             return \"Prediction output format unrecognized.\"\n",
    "\n",
    "#     def tool_is_used(self, query):\n",
    "#         return \"```ClearanceHepatocyteAZPred\" in query\n",
    "\n",
    "#     def process_query(self, query):\n",
    "#         return extract_prompt(query, word=\"ClearanceHepatocyteAZPred\")\n",
    "\n",
    "#     def instructions(self):\n",
    "#         return (\n",
    "#             \"=== Clearance Hepatocyte AZ Prediction Instructions ===\\n\"\n",
    "#             \"This tool predicts the hepatocyte clearance of a small molecule (given as SMILES).\\n\\n\"\n",
    "#             \"To use this tool, invoke it exactly like this:\\n\"\n",
    "#             \"```ClearanceHepatocyteAZPred\\n\"\n",
    "#             \"{Drug SMILES}\\n\"\n",
    "#             \"```\\n\\n\"\n",
    "#             \"• **Keyword**: `ClearanceHepatocyteAZPred` (must match exactly).\\n\"\n",
    "#             \"• **Line 2**: the SMILES string of your ligand.\\n\\n\"\n",
    "#             \"**Example:**\\n\"\n",
    "#             \"```ClearanceHepatocyteAZPred\\n\"\n",
    "#             \"CC(=O)Oc1ccccc1C(=O)O\\n\"\n",
    "#             \"```\\n\"\n",
    "#             \"This will return a predicted hepatocyte clearance value for that molecule.\\n\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29419036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearancePred = ClearanceHepatocyteAZPred()\n",
    "\n",
    "# smiles = \"COc1c(F)cccc1C1=C(c2ccc(O[C@H]3CCN(CCCF)C3)cc2)c2ccc(O)cc2CCC1\"\n",
    "# prediction_clearance = clearancePred.use_tool(smiles)\n",
    "# print(prediction_clearance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36ba8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClearanceMicrosomeAZPred:\n",
    "#     def __init__(self):\n",
    "#         self.tool_name = \"Clearance Microsome AZ Prediction\"\n",
    "\n",
    "#     def use_tool(self, smiles_string):\n",
    "#         # Assumes tdc_prompts_json has an entry \"ClearanceMicrosomeAZ\" for this task\n",
    "#         prediction = txgemma_predict(\n",
    "#             tdc_prompts_json[\"Clearance_Microsome_AZ\"].replace(\"{Drug SMILES}\", smiles_string)\n",
    "#         )\n",
    "#         # Example output might be: \"Answer: Clearance rate: 45.6\"\n",
    "#         match = re.search(r\"Answer:*([0-9]*\\.?[0-9]+)\", prediction)\n",
    "        \n",
    "#         clearance_value = float(match.group(1))\n",
    "#         # You can adjust thresholds or interpretation as needed\n",
    "#         return f\"{smiles_string} has a predicted microsomal clearance rate of {clearance_value} (mL·min⁻¹·g⁻¹).\"\n",
    "\n",
    "#     def tool_is_used(self, query):\n",
    "#         # Check for exact keyword in query\n",
    "#         return \"```ClearanceMicrosomeAZPred\" in query\n",
    "\n",
    "#     def process_query(self, query):\n",
    "#         # Clean query to remove tool call block and extract prompt\n",
    "#         return extract_prompt(query, word=\"ClearanceMicrosomeAZPred\")\n",
    "\n",
    "#     def instructions(self):\n",
    "#         return (\n",
    "#             \"=== Clearance Microsome AZ Prediction Instructions ===\\n\"\n",
    "#             \"This tool predicts the microsomal clearance rate of a small molecule (given as SMILES),\\n\"\n",
    "#             \"based on AstraZeneca data and models.\\n\\n\"\n",
    "#             \"To use this tool, invoke it exactly like this:\\n\"\n",
    "#             \"```ClearanceMicrosomeAZPred\\n\"\n",
    "#             \"{Drug SMILES}\\n\"\n",
    "#             \"```\\n\\n\"\n",
    "#             \"• **Keyword**: `ClearanceMicrosomeAZPred` (must match exactly).\\n\"\n",
    "#             \"• **Line 2**: the SMILES string of your molecule.\\n\\n\"\n",
    "#             \"**Example:**\\n\"\n",
    "#             \"```ClearanceMicrosomeAZPred\\n\"\n",
    "#             \"CC(=O)Oc1ccccc1C(=O)O\\n\"\n",
    "#             \"```\\n\"\n",
    "#             \"This will return a predicted microsomal clearance rate for that molecule.\\n\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f9bf0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearance_pred = ClearanceMicrosomeAZPred()\n",
    "\n",
    "# smiles = \"COc1c(F)cccc1C1=C(c2ccc(O[C@H]3CCN(CCCF)C3)cc2)c2ccc(O)cc2CCC1\"\n",
    "# prediction_clearance = clearance_pred.use_tool(smiles)\n",
    "# print(prediction_clearance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "011d0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfLifeObachPred:\n",
    "    def __init__(self):\n",
    "        self.tool_name = \"Half-Life Prediction (Obach)\"\n",
    "\n",
    "    def use_tool(self, smiles_string):\n",
    "        # Assuming txgemma_predict and tdc_prompts_json have an entry for \"HalfLifeObach\"\n",
    "        prediction = txgemma_predict(\n",
    "            tdc_prompts_json[\"Half_Life_Obach\"].replace(\"{Drug SMILES}\", smiles_string)\n",
    "        )\n",
    "        # Example output might be: \"Answer: Half-life (hours): 4.2\"\n",
    "        match = re.search(r\"Answer:\\s*(?:Half-life \\(hours\\):\\s*)?([0-9]*\\.?[0-9]+)\", prediction)\n",
    "        \n",
    "        if match:\n",
    "            half_life = float(match.group(1))\n",
    "            return f\"{smiles_string} is predicted to have a half-life of {half_life:.2f} hours.\"\n",
    "        else:\n",
    "            return \"Prediction output format unrecognized.\"\n",
    "\n",
    "    def tool_is_used(self, query):\n",
    "        # Check for exact keyword in query\n",
    "        return \"```HalfLifeObachPred\" in query\n",
    "\n",
    "    def process_query(self, query):\n",
    "        # Clean query to remove tool call block and extract prompt\n",
    "        return extract_prompt(query, word=\"HalfLifeObachPred\")\n",
    "\n",
    "    def instructions(self):\n",
    "        return (\n",
    "            \"=== Half-Life Prediction (Obach) Instructions ===\\n\"\n",
    "            \"This tool predicts the half-life (in hours) of a small molecule (given as SMILES),\\n\"\n",
    "            \"based on the Obach model.\\n\\n\"\n",
    "            \"To use this tool, invoke it exactly like this:\\n\"\n",
    "            \"```HalfLifeObachPred\\n\"\n",
    "            \"{Drug SMILES}\\n\"\n",
    "            \"```\\n\\n\"\n",
    "            \"• **Keyword**: `HalfLifeObachPred` (must match exactly).\\n\"\n",
    "            \"• **Line 2**: the SMILES string of your molecule.\\n\\n\"\n",
    "            \"**Example:**\\n\"\n",
    "            \"```HalfLifeObachPred\\n\"\n",
    "            \"CC(=O)Oc1ccccc1C(=O)O\\n\"\n",
    "            \"```\\n\"\n",
    "            \"This will return a predicted half-life (in hours) for that molecule.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3c7ecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COc1c(F)cccc1C1=C(c2ccc(O[C@H]3CCN(CCCF)C3)cc2)c2ccc(O)cc2CCC1 is predicted to have a half-life of 10.00 hours.\n"
     ]
    }
   ],
   "source": [
    "half_life_pred = HalfLifeObachPred()\n",
    "# Only pass the SMILES string, since AA_sequence is not needed here\n",
    "smiles = \"COc1c(F)cccc1C1=C(c2ccc(O[C@H]3CCN(CCCF)C3)cc2)c2ccc(O)cc2CCC1\"\n",
    "prediction_half_life = half_life_pred.use_tool(smiles)\n",
    "print(prediction_half_life)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a3b9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDssLombardoPred:\n",
    "    def __init__(self):\n",
    "        self.tool_name = \"VDss Lombardo Prediction\"\n",
    "\n",
    "    def use_tool(self, smiles_string):\n",
    "        # Assuming txgemma_predict and tdc_prompts_json have an entry for VDss Lombardo\n",
    "        prediction = txgemma_predict(\n",
    "            tdc_prompts_json[\"VDss_Lombardo\"].replace(\"{Drug SMILES}\", smiles_string)\n",
    "        )\n",
    "        # Example output might be: \"Answer: VDss (L/kg): 0.85\"\n",
    "        match = re.search(r\"Answer:\\s*(?:VDss\\s*\\(L/kg\\):\\s*)?([0-9]*\\.?[0-9]+)\", prediction)\n",
    "\n",
    "        if match:\n",
    "            vdss_value = float(match.group(1))\n",
    "            # Interpret vdss_value as you prefer, e.g.:\n",
    "            return f\"{smiles_string} has a predicted VDss of {vdss_value:.2f}(L/kg).\"\n",
    "        else:\n",
    "            return \"Prediction output format unrecognized.\"\n",
    "\n",
    "    def tool_is_used(self, query):\n",
    "        # Check for exact keyword in query\n",
    "        return \"```VDssLombardoPred\" in query\n",
    "\n",
    "    def process_query(self, query):\n",
    "        # Clean query to remove tool call block and extract prompt\n",
    "        return extract_prompt(query, word=\"VDssLombardoPred\")\n",
    "\n",
    "    def instructions(self):\n",
    "        return (\n",
    "            \"=== VDss Lombardo Prediction Instructions ===\\n\"\n",
    "            \"This tool predicts the steady-state volume of distribution (VDss) in L/kg\\n\"\n",
    "            \"for a small molecule using the Lombardo method.\\n\\n\"\n",
    "            \"To use this tool, invoke it exactly like this:\\n\"\n",
    "            \"```VDssLombardoPred\\n\"\n",
    "            \"{Drug SMILES}\\n\"\n",
    "            \"```\\n\\n\"\n",
    "            \"• **Keyword**: `VDssLombardoPred` (must match exactly).\\n\"\n",
    "            \"• **Line 2**: the SMILES string of your molecule.\\n\\n\"\n",
    "            \"**Example:**\\n\"\n",
    "            \"```VDssLombardoPred\\n\"\n",
    "            \"CC(=O)Oc1ccccc1C(=O)O\\n\"\n",
    "            \"```\\n\"\n",
    "            \"This will return the predicted VDss value for that molecule.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89981b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COc1c(F)cccc1C1=C(c2ccc(O[C@H]3CCN(CCCF)C3)cc2)c2ccc(O)cc2CCC1 has a predicted VDss of 3.00(L/kg).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the VDss Lombardo predictor\n",
    "vdssPred = VDssLombardoPred()\n",
    "\n",
    "# Use only the SMILES string since VDssLombardoPred takes just that\n",
    "smiles = \"COc1c(F)cccc1C1=C(c2ccc(O[C@H]3CCN(CCCF)C3)cc2)c2ccc(O)cc2CCC1\"\n",
    "prediction_vdss = vdssPred.use_tool(smiles)\n",
    "print(prediction_vdss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32011176",
   "metadata": {},
   "source": [
    "# PubMed search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b957c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --quiet biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b531ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Medline, Entrez\n",
    "\n",
    "# This class will allow us to interface with PubMed\n",
    "class PubMedSearch:\n",
    "    def __init__(self):\n",
    "      self.tool_name = \"PubMed Search\"\n",
    "\n",
    "    def tool_is_used(self, query: str):\n",
    "        # This just checks to see if the tool call was evoked\n",
    "        return \"```PubMedSearch\" in query\n",
    "\n",
    "    def process_query(self, query: str):\n",
    "        # Here, we clean to query to remove the tool call\n",
    "        search_text = extract_prompt(query, word=\"PubMedSearch\")\n",
    "        return search_text.strip()\n",
    "\n",
    "    def use_tool(self, search_text):\n",
    "        # Here, we are searching through PubMed and returning relevant articles\n",
    "        pmids = list()\n",
    "        handle = Entrez.esearch(db=\"pubmed\", sort=\"relevance\", term=search_text, retmax=3)\n",
    "        record = Entrez.read(handle)\n",
    "        pmids = record.get(\"IdList\", [])\n",
    "        handle.close()\n",
    "\n",
    "        if not pmids:\n",
    "            return f\"No PubMed articles found for '{search_text}' Please try a simpler search query.\"\n",
    "\n",
    "        fetch_handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pmids), rettype=\"medline\", retmode=\"text\")\n",
    "        records = list(Medline.parse(fetch_handle))\n",
    "        fetch_handle.close()\n",
    "\n",
    "        result_str = f\"=== PubMed Search Results for: '{search_text}' ===\\n\"\n",
    "        for i, record in enumerate(records, start=1):\n",
    "            pmid = record.get(\"PMID\", \"N/A\")\n",
    "            title = record.get(\"TI\", \"No title available\")\n",
    "            abstract = record.get(\"AB\", \"No abstract available\")\n",
    "            journal = record.get(\"JT\", \"No journal info\")\n",
    "            pub_date = record.get(\"DP\", \"No date info\")\n",
    "            authors = record.get(\"AU\", [])\n",
    "            authors_str = \", \".join(authors[:3])\n",
    "            result_str += (\n",
    "                f\"\\n--- Article #{i} ---\\n\"\n",
    "                f\"PMID: {pmid}\\n\"\n",
    "                f\"Title: {title}\\n\"\n",
    "                f\"Authors: {authors_str}\\n\"\n",
    "                f\"Journal: {journal}\\n\"\n",
    "                f\"Publication Date: {pub_date}\\n\"\n",
    "                f\"Abstract: {abstract}\\n\")\n",
    "        return f\"Query: {search_text}\\nResults: {result_str}\"\n",
    "\n",
    "    def instructions(self):\n",
    "        # Here, we are explaining how the tool works to the agent\n",
    "        return (\n",
    "            f\"{'@' * 10}\\n@@@ PubMed Search Tool Instructions @@@\\n\\n\"\n",
    "            \"### What This Tool Does\\n\"\n",
    "            \"The PubMed Search Tool queries the NCBI Entrez API (PubMed) for a given search phrase, \"\n",
    "            \"and retrieves metadata for a few of the top articles (PMID, title, authors, journal, date, abstract).\\n\\n\"\n",
    "            \"### When / Why You Should Use It\\n\"\n",
    "            \"- To find **scientific literature** references on a specific biomedical topic.\\n\"\n",
    "            \"- To retrieve **abstracts, titles, authors**, and other metadata.\\n\\n\"\n",
    "            \"### Query Format\\n\"\n",
    "            \"Wrap your request with triple backticks, starting with `PubMedSearch`. For example:\\n\\n\"\n",
    "            \"```PubMedSearch\\ncancer immunotherapy\\n```\\n\\n\"\n",
    "            \"### Example\\n\"\n",
    "            \"```PubMedSearch\\nmachine learning in drug discovery\\n```\\n\"\n",
    "            \"- This will search PubMed for articles related to 'machine learning in drug discovery', \"\n",
    "            \"fetch up to 3 PMIDs, and return their titles, abstracts, etc.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31feb57f",
   "metadata": {},
   "source": [
    "# Wrapping it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529e105",
   "metadata": {},
   "source": [
    "### Creating a tool manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "237d2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tool manager will hold all of the tools, and provide an interface for the agent\n",
    "class ToolManager:\n",
    "    def __init__(self, toolset):\n",
    "        self.toolset = toolset\n",
    "\n",
    "    def tool_prompt(self):\n",
    "        # This will let the agent know what tools it has access to\n",
    "        tool_names = \", \".join([tool.tool_name for tool in self.toolset])\n",
    "        return f\"You have access to the following tools: {tool_names}\\n{self.tool_instructions()}. You can only use one tool at a time. These are the only tools you have access to nothing else.\"\n",
    "\n",
    "    def tool_instructions(self):\n",
    "        # This allows the agent to know how to use the tools\n",
    "        tool_instr = \"\\n\".join([tool.instructions() for tool in self.toolset])\n",
    "        return f\"The following is a set of instructions on how to use each tool.\\n{tool_instr}\"\n",
    "\n",
    "    def use_tool(self, query):\n",
    "        # This will iterate through all of the tools\n",
    "        # and find the correct tool that the agent requested\n",
    "        for tool in self.toolset:\n",
    "            if tool.tool_is_used(query):\n",
    "                # use the tool and return the output\n",
    "                return tool.use_tool(tool.process_query(query))\n",
    "        return f\"No tool match for search: {query}\"\n",
    "\n",
    "if USE_CHAT:\n",
    "    tools = ToolManager([TxGemmaChatTool(), BioavailabilityPred(), HalfLifeObachPred(), VDssLombardoPred(), PubMedSearch()])\n",
    "else:\n",
    "    tools = ToolManager([BioavailabilityPred(), HalfLifeObachPred(), VDssLombardoPred(), PubMedSearch()])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26c65b",
   "metadata": {},
   "source": [
    "### Creating a Gemini inference tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62f0221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "def inference_gemini(prompt, system_prompt, model_str):\n",
    "    if model_str == \"gemini-2.5-flash\":\n",
    "        model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-2.5-flash-preview-05-20\",\n",
    "            system_instruction=system_prompt\n",
    "        )\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    raise ValueError(f\"Unsupported model string: {model_str}\")\n",
    "\n",
    "def safe_inference(prompt, system_prompt, model_str, retries=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return inference_gemini(prompt, system_prompt, model_str)\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"[Attempt {attempt + 1}] Error: {error_msg}\")\n",
    "\n",
    "            delay_match = re.search(r\"retry_delay {\\s*seconds: (\\d+)\", error_msg)\n",
    "            if delay_match:\n",
    "                wait_time = int(delay_match.group(1))\n",
    "            elif \"ResourceExhausted\" in error_msg:\n",
    "                wait_time = 60\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Waiting {wait_time} seconds before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"Max retries reached. Raising exception.\")\n",
    "                raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76da41",
   "metadata": {},
   "source": [
    "# Creating a therapeutics agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticTx:\n",
    "    def __init__(self, tool_manager, model_str, num_steps=5):\n",
    "        self.curr_steps = 0\n",
    "        self.num_steps = num_steps\n",
    "        self.model_str = model_str\n",
    "        self.tool_manager = tool_manager\n",
    "        self.thoughts = []\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_steps = 0\n",
    "        self.thoughts.clear()\n",
    "        self.actions.clear()\n",
    "        self.observations.clear()\n",
    "\n",
    "    def system_prompt(self, use_tools=True):\n",
    "        role_prompt = \"You are an expert therapeutic agent. You answer accurately and thoroughly.\"\n",
    "        prev_actions = f\"You can perform a maximum of {self.num_steps} actions. You have performed {self.curr_steps} and have {self.num_steps - self.curr_steps - 1} left.\"\n",
    "        tool_prompt = (\"You can use tools to solve problems and answer questions. \" + self.tool_manager.tool_prompt()) if use_tools else \"You cannot use any tools right now.\"\n",
    "        return f\"{role_prompt} {prev_actions} {tool_prompt}\"\n",
    "\n",
    "    def prior_information(self, query):\n",
    "        info_txt = f\"Question: {query}\\n\" if query else \"\"\n",
    "        for i in range(self.curr_steps):\n",
    "            info_txt += f\"### Thought {i + 1}: {self.thoughts[i]}\\n\"\n",
    "            info_txt += f\"### Action {i + 1}: {self.actions[i]}\\n\"\n",
    "            info_txt += f\"### Observation {i + 1}: {self.observations[i]}\\n\\n\"\n",
    "            info_txt += \"@\" * 20\n",
    "        return info_txt\n",
    "\n",
    "    def step(self, question):\n",
    "        self.reset()\n",
    "        for _ in range(self.num_steps):\n",
    "            if self.curr_steps == self.num_steps - 1:\n",
    "                return safe_inference(\n",
    "                    prompt=f\"{self.prior_information(question)}\\nYou must now provide an answer to this question {question}\",\n",
    "                    system_prompt=self.system_prompt(use_tools=False),\n",
    "                    model_str=self.model_str\n",
    "                )\n",
    "            else:\n",
    "                thought = safe_inference(\n",
    "                    prompt=f\"{self.prior_information(question)}\\nYou cannot currently use tools but you can think about the problem and what tools you want to use. This was the question, think about plans for how to use tools to answer this {question}. Let's think step by step (respond with only 1-2 sentences).\\nThought: \",\n",
    "                    system_prompt=self.system_prompt(use_tools=False),\n",
    "                    model_str=self.model_str\n",
    "                )\n",
    "                action = safe_inference(\n",
    "                    prompt=f\"{self.prior_information(question)}\\n{thought}\\nNow you must use tools to answer the following user query [{question}], closely following the tool instructions. Tool\",\n",
    "                    system_prompt=self.system_prompt(use_tools=True),\n",
    "                    model_str=self.model_str\n",
    "                )\n",
    "                obs = self.tool_manager.use_tool(action)\n",
    "\n",
    "                print(\"Thought:\", thought)\n",
    "                print(\"Action:\", action)\n",
    "                print(\"Observation:\", obs)\n",
    "\n",
    "                self.thoughts.append(thought)\n",
    "                self.actions.append(action)\n",
    "                self.observations.append(obs)\n",
    "                self.curr_steps += 1\n",
    "\n",
    "# Initialize agent\n",
    "agentictx = AgenticTx(tool_manager=tools, model_str=\"gemini-2.5-flash\")\n",
    "\n",
    "# File paths\n",
    "input_file = \"Lipinski_after.txt\"\n",
    "output_file = \"Lipinski_after_pk_checked.txt\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Input file '{input_file}' not found.\")\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    count = 0\n",
    "    for line in infile:\n",
    "        smiles = line.strip()\n",
    "        if not smiles or smiles.startswith(\"#\"):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing SMILES #{count + 1}: {smiles}\")\n",
    "        question = f\"What are the PK properties of this drug {smiles}?\"\n",
    "\n",
    "        try:\n",
    "            response = agentictx.step(question)\n",
    "            outfile.write(f\"SMILES: {smiles}\\nResponse: {response}\\n{'=' * 60}\\n\")\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            outfile.write(f\"SMILES: {smiles}\\nError: {e}\\n{'=' * 60}\\n\")\n",
    "\n",
    "print(f\"\\nPK property analysis completed. {count} SMILES processed. Results saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102327f",
   "metadata": {},
   "source": [
    "# Visualizing the results in 2D\n",
    "input: Lipinski_after.txt\n",
    "output: rank_{rank}_ligand_{ligand_number}.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "308dc4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the results in 2D\n",
    "\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import os\n",
    "\n",
    "# Load the combined data file\n",
    "input_file = r'Lipinski_after.txt'\n",
    "df = pd.read_csv(input_file, sep='\\t')\n",
    "\n",
    "# Output directory for images\n",
    "output_dir = r'ligand_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate and save images with rank-based filenames\n",
    "for _, row in df.iterrows():\n",
    "    rank = row['Rank']\n",
    "    ligand_number = row['Ligand Number']\n",
    "    confidence = row['Confidence Score']\n",
    "    smiles = row['SMILES']\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        img = Draw.MolToImage(\n",
    "            mol, size=(300, 300),\n",
    "            legend=f'Rank: {rank}\\nLigand: {ligand_number}\\nScore: {confidence:.4f}'\n",
    "        )\n",
    "        filename = f'rank_{rank}_ligand_{ligand_number}.png'\n",
    "        img.save(os.path.join(output_dir, filename))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
